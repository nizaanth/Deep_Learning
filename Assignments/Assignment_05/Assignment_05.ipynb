{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a10r9fsDpg_5"
   },
   "source": [
    "# **Course**: Deep Learning\n",
    "\n",
    "[<img align=\"right\" width=\"400\" height=\"100\" src=\"https://www.tu-braunschweig.de/typo3conf/ext/tu_braunschweig/Resources/Public/Images/Logos/tu_braunschweig_logo.svg\">](https://www.tu-braunschweig.de/en/) \n",
    "\n",
    "[Mehdi Maboudi](https://www.tu-braunschweig.de/en/igp/staff/mehdi-maboudi) \\([m.maboudi@tu-bs.de](m.maboudi@tu-bs.de)) and [Pedro Achanccaray](https://www.tu-braunschweig.de/en/igp/staff/pedro-diaz) (p.diaz@tu-bs.de)\n",
    "\n",
    "[Technical University of Braunschweig](https://www.tu-braunschweig.de/en/)  \n",
    "[Institute of Geodesy and Photogrammetry](https://www.tu-braunschweig.de/igp) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ojJa7RUApi3L"
   },
   "source": [
    "# **Assignment 05:** Transfer learning and Fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mJy_gXvGpyfu"
   },
   "source": [
    "In this assignment you will explore the differences between transfer learning and fine tuning.\n",
    "\n",
    "For this, you will use the **VGG16** pre-trained network with **ImageNet** dataset.\n",
    "\n",
    "<center>\n",
    "<img width=600 src=\"https://miro.medium.com/max/1400/1*NNifzsJ7tD2kAfBXt3AzEg.png\" img>\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0__nx5Zg4G9Y"
   },
   "source": [
    "- **Transfer learning:** \n",
    "  1. Take a pre-trained model as $base\\_model$\n",
    "  2. Freeze the $base\\_model$\n",
    "  3. Add a $head$ (classification layers) to the $base\\_model$\n",
    "  4. Train the new model\n",
    "  5. $trainable\\_parameters = head\\_parameters $\n",
    "\n",
    "- **Fine tuning:**\n",
    "  1. Take a pre-trained model as $base\\_model$\n",
    "  2. Freeze some layers of the $base\\_model$\n",
    "  3. Add a $head$ (classification layers) to the $base\\_model$\n",
    "  4. Train the new model\n",
    "  5. $trainable\\_parameters = head\\_parameters + base\\_model\\_parameters $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o0JRDMT9yuRd"
   },
   "source": [
    "## **PyTorch**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dTbDgDzUv0fR"
   },
   "source": [
    "For PyTorch, you can visit the following links:\n",
    "- [VGG16 PyTorch documentation](https://pytorch.org/vision/main/models/generated/torchvision.models.vgg16.html)\n",
    "- [Transfer learning with PyTorch](https://debuggercafe.com/transfer-learning-with-pytorch/)\n",
    "- [Freeze layers of PyTorch model](https://medium.com/@shuklaatul032/freeze-layers-of-pytorch-model-48d2725223b3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GofutGvjtSOZ"
   },
   "source": [
    "## **Load packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "-ulQ-h_4pW6J"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Management of files\n",
    "import os\n",
    "from os.path import exists, join\n",
    "\n",
    "# Tensorflow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, \\\n",
    "                                       EarlyStopping\n",
    "\n",
    "# Monitor training\n",
    "import wandb\n",
    "from wandb.integration.keras import WandbMetricsLogger\n",
    "\n",
    "# Working with arrays\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# External files with functions to load the dataset,\n",
    "# create a CNN model, and a data generator.\n",
    "from importlib import reload\n",
    "import datasets\n",
    "import data_generator\n",
    "# Useful to reload modified external files without need\n",
    "# of restarting the kernel. Just run again this cell.\n",
    "reload(datasets)\n",
    "reload(data_generator)\n",
    "\n",
    "from datasets import *\n",
    "from models import *\n",
    "from data_generator import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1lHJ0P-gtWmW"
   },
   "source": [
    "## **Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "1ptI9WsMsYt6"
   },
   "outputs": [],
   "source": [
    "def freeze_up_to(model, freeze_layer_name):\n",
    "  \"\"\"Function to freeze some layers of the model\n",
    "\n",
    "  Args:\n",
    "      model (keras.Model): a keras.Model\n",
    "      freeze_layer_name (str): layer name of \"model\". All layers up\n",
    "        to this layer will be freezed.\n",
    "\n",
    "  Returns:\n",
    "      keras.Model: a keras.Model with some layers freezed.\n",
    "  \"\"\"\n",
    "  # Getting layer number based on layer name\n",
    "  for id_layer, layer in enumerate(model.layers):\n",
    "    if layer.name == freeze_layer_name:\n",
    "      layer_number = id_layer\n",
    "      break\n",
    "\n",
    "  # Froze layers\n",
    "  for layer in model.layers[:layer_number]:\n",
    "    layer.trainable = False\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kanUzPrltfD4"
   },
   "source": [
    "## **Base models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H0icG9OIvHcc"
   },
   "source": [
    "### **VGG16** model with top layers (classification layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2mBtSBH7u-Fp",
    "outputId": "90967165-3ea8-410d-b129-2e0cb19f2c3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_13 (InputLayer)       [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 4096)              102764544 \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 4096)              16781312  \n",
      "                                                                 \n",
      " predictions (Dense)         (None, 1000)              4097000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 138357544 (527.79 MB)\n",
      "Trainable params: 138357544 (527.79 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg16_full = VGG16(include_top=True,\n",
    "                   weights=\"imagenet\",      \n",
    "                   input_shape=(224, 224, 3))\n",
    "\n",
    "vgg16_full.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yYyU6UOwy4kK"
   },
   "source": [
    "### **VGG16** model without top layers (classification layers): feature extractor, and its original input shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fM5ogDeGtgl1",
    "outputId": "24d4ddb1-1a5a-44d1-9351-e7d800296f43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_16 (InputLayer)       [(None, 64, 64, 3)]       0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 64, 64, 64)        1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 64, 64, 64)        36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 32, 32, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 32, 32, 128)       73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 32, 32, 128)       147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 16, 16, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 16, 16, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 16, 16, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 8, 8, 512)         1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 8, 8, 512)         2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 8, 8, 512)         2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 4, 4, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14714688 (56.13 MB)\n",
      "Trainable params: 14714688 (56.13 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg16_base = VGG16(include_top=False,\n",
    "                   weights=\"imagenet\",      \n",
    "                   input_shape=(64,64,3))\n",
    "\n",
    "vgg16_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EbJM8HtezDHg"
   },
   "source": [
    "### **VGG16** model without top layers (classification layers): feature extractor, and a different input shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yd5CC8M6ul4C",
    "outputId": "75a5021b-c34e-4fc0-e664-1b0d784917ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_15 (InputLayer)       [(None, 64, 64, 3)]       0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 64, 64, 64)        1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 64, 64, 64)        36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 32, 32, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 32, 32, 128)       73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 32, 32, 128)       147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 16, 16, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 16, 16, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 16, 16, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 8, 8, 512)         1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 8, 8, 512)         2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 8, 8, 512)         2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 4, 4, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14714688 (56.13 MB)\n",
      "Trainable params: 14714688 (56.13 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg16_dif_input = VGG16(include_top=False,\n",
    "                        weights=\"imagenet\",      \n",
    "                        input_shape=(64,64,3))\n",
    "\n",
    "vgg16_dif_input.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_DIR = \".\" # os.getcwd()\n",
    "SEED = 42\n",
    "BATCH_SIZE = 32\n",
    "TARGET_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_dataset = \"https://zenodo.org/record/7711810/files/EuroSAT_RGB.zip?download=1\"\n",
    "filename = \"EuroSAT_RGB.zip\"\n",
    "\n",
    "if not exists(\"EuroSAT_RGB\"):\n",
    "  !pip install wget\n",
    "  import wget\n",
    "  f = wget.download(url_dataset, PROJECT_DIR)\n",
    "  import zipfile\n",
    "  with zipfile.ZipFile(filename, \"r\") as zip_ref:\n",
    "    zip_ref.extractall(\".\")\n",
    "  os.remove(join(PROJECT_DIR, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path_image</th>\n",
       "      <th>class_str</th>\n",
       "      <th>class_int</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.\\EuroSAT_RGB\\Forest\\Forest_2313.jpg</td>\n",
       "      <td>Forest</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>.\\EuroSAT_RGB\\PermanentCrop\\PermanentCrop_2358...</td>\n",
       "      <td>PermanentCrop</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>.\\EuroSAT_RGB\\HerbaceousVegetation\\HerbaceousV...</td>\n",
       "      <td>HerbaceousVegetation</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>.\\EuroSAT_RGB\\Pasture\\Pasture_1415.jpg</td>\n",
       "      <td>Pasture</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.\\EuroSAT_RGB\\Highway\\Highway_1611.jpg</td>\n",
       "      <td>Highway</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26995</th>\n",
       "      <td>.\\EuroSAT_RGB\\River\\River_76.jpg</td>\n",
       "      <td>River</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26996</th>\n",
       "      <td>.\\EuroSAT_RGB\\Forest\\Forest_2391.jpg</td>\n",
       "      <td>Forest</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26997</th>\n",
       "      <td>.\\EuroSAT_RGB\\AnnualCrop\\AnnualCrop_861.jpg</td>\n",
       "      <td>AnnualCrop</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26998</th>\n",
       "      <td>.\\EuroSAT_RGB\\Pasture\\Pasture_1796.jpg</td>\n",
       "      <td>Pasture</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26999</th>\n",
       "      <td>.\\EuroSAT_RGB\\River\\River_2155.jpg</td>\n",
       "      <td>River</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              path_image  \\\n",
       "0                   .\\EuroSAT_RGB\\Forest\\Forest_2313.jpg   \n",
       "1      .\\EuroSAT_RGB\\PermanentCrop\\PermanentCrop_2358...   \n",
       "2      .\\EuroSAT_RGB\\HerbaceousVegetation\\HerbaceousV...   \n",
       "3                 .\\EuroSAT_RGB\\Pasture\\Pasture_1415.jpg   \n",
       "4                 .\\EuroSAT_RGB\\Highway\\Highway_1611.jpg   \n",
       "...                                                  ...   \n",
       "26995                   .\\EuroSAT_RGB\\River\\River_76.jpg   \n",
       "26996               .\\EuroSAT_RGB\\Forest\\Forest_2391.jpg   \n",
       "26997        .\\EuroSAT_RGB\\AnnualCrop\\AnnualCrop_861.jpg   \n",
       "26998             .\\EuroSAT_RGB\\Pasture\\Pasture_1796.jpg   \n",
       "26999                 .\\EuroSAT_RGB\\River\\River_2155.jpg   \n",
       "\n",
       "                  class_str  class_int  \n",
       "0                    Forest          1  \n",
       "1             PermanentCrop          6  \n",
       "2      HerbaceousVegetation          2  \n",
       "3                   Pasture          5  \n",
       "4                   Highway          3  \n",
       "...                     ...        ...  \n",
       "26995                 River          8  \n",
       "26996                Forest          1  \n",
       "26997            AnnualCrop          0  \n",
       "26998               Pasture          5  \n",
       "26999                 River          8  \n",
       "\n",
       "[27000 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_data = join(PROJECT_DIR, \"EuroSAT_RGB\")\n",
    "\n",
    "df, n_classes = read_eurosat(path_data=path_data, SEED=SEED)\n",
    "classes = np.unique(df[\"class_str\"].values)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Train, Validation and Test sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples per class - train: [1800 1800 1800 1500 1500 1200 1500 1800 1500 1800]\n",
      "Samples per class - val: [600 600 600 500 500 400 500 600 500 600]\n",
      "Samples per class - test: [600 600 600 500 500 400 500 600 500 600]\n"
     ]
    }
   ],
   "source": [
    "splits = train_val_test_split(df,\n",
    "                              val_size=0.2,\n",
    "                              test_size=0.2,\n",
    "                              SEED=SEED,\n",
    "                             )\n",
    "\n",
    "x_train = splits[\"x_train\"]\n",
    "y_train = splits[\"y_train\"]\n",
    "x_val = splits[\"x_val\"]\n",
    "y_val = splits[\"y_val\"]\n",
    "x_test = splits[\"x_test\"]\n",
    "y_test = splits[\"y_test\"]\n",
    "\n",
    "#SANITY CHECK\n",
    "\n",
    "# Number of samples per class\n",
    "_, counts_train = np.unique(y_train, return_counts=True)\n",
    "_, counts_val = np.unique(y_val, return_counts=True)\n",
    "_, counts_test = np.unique(y_test, return_counts=True)\n",
    "\n",
    "print(\"Samples per class - train: {}\".format(counts_train))\n",
    "print(\"Samples per class - val: {}\".format(counts_val))\n",
    "print(\"Samples per class - test: {}\".format(counts_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data generator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (32, 64, 64, 3) (32, 10)\n",
      "1 (32, 64, 64, 3) (32, 10)\n",
      "2 (32, 64, 64, 3) (32, 10)\n",
      "3 (32, 64, 64, 3) (32, 10)\n",
      "4 (32, 64, 64, 3) (32, 10)\n",
      "5 (32, 64, 64, 3) (32, 10)\n",
      "6 (32, 64, 64, 3) (32, 10)\n",
      "7 (32, 64, 64, 3) (32, 10)\n",
      "8 (32, 64, 64, 3) (32, 10)\n",
      "9 (32, 64, 64, 3) (32, 10)\n",
      "10 (32, 64, 64, 3) (32, 10)\n",
      "11 (32, 64, 64, 3) (32, 10)\n",
      "12 (32, 64, 64, 3) (32, 10)\n",
      "13 (32, 64, 64, 3) (32, 10)\n",
      "14 (32, 64, 64, 3) (32, 10)\n",
      "15 (32, 64, 64, 3) (32, 10)\n",
      "16 (32, 64, 64, 3) (32, 10)\n",
      "17 (32, 64, 64, 3) (32, 10)\n",
      "18 (32, 64, 64, 3) (32, 10)\n",
      "19 (32, 64, 64, 3) (32, 10)\n",
      "20 (32, 64, 64, 3) (32, 10)\n",
      "21 (32, 64, 64, 3) (32, 10)\n",
      "22 (32, 64, 64, 3) (32, 10)\n",
      "23 (32, 64, 64, 3) (32, 10)\n",
      "24 (32, 64, 64, 3) (32, 10)\n",
      "25 (32, 64, 64, 3) (32, 10)\n",
      "26 (32, 64, 64, 3) (32, 10)\n",
      "27 (32, 64, 64, 3) (32, 10)\n",
      "28 (32, 64, 64, 3) (32, 10)\n",
      "29 (32, 64, 64, 3) (32, 10)\n",
      "30 (32, 64, 64, 3) (32, 10)\n",
      "31 (32, 64, 64, 3) (32, 10)\n",
      "32 (32, 64, 64, 3) (32, 10)\n",
      "33 (32, 64, 64, 3) (32, 10)\n",
      "34 (32, 64, 64, 3) (32, 10)\n",
      "35 (32, 64, 64, 3) (32, 10)\n",
      "36 (32, 64, 64, 3) (32, 10)\n",
      "37 (32, 64, 64, 3) (32, 10)\n",
      "38 (32, 64, 64, 3) (32, 10)\n",
      "39 (32, 64, 64, 3) (32, 10)\n",
      "40 (32, 64, 64, 3) (32, 10)\n",
      "41 (32, 64, 64, 3) (32, 10)\n",
      "42 (32, 64, 64, 3) (32, 10)\n",
      "43 (32, 64, 64, 3) (32, 10)\n",
      "44 (32, 64, 64, 3) (32, 10)\n",
      "45 (32, 64, 64, 3) (32, 10)\n",
      "46 (32, 64, 64, 3) (32, 10)\n",
      "47 (32, 64, 64, 3) (32, 10)\n",
      "48 (32, 64, 64, 3) (32, 10)\n",
      "49 (32, 64, 64, 3) (32, 10)\n",
      "50 (32, 64, 64, 3) (32, 10)\n",
      "51 (32, 64, 64, 3) (32, 10)\n",
      "52 (32, 64, 64, 3) (32, 10)\n",
      "53 (32, 64, 64, 3) (32, 10)\n",
      "54 (32, 64, 64, 3) (32, 10)\n",
      "55 (32, 64, 64, 3) (32, 10)\n",
      "56 (32, 64, 64, 3) (32, 10)\n",
      "57 (32, 64, 64, 3) (32, 10)\n",
      "58 (32, 64, 64, 3) (32, 10)\n",
      "59 (32, 64, 64, 3) (32, 10)\n",
      "60 (32, 64, 64, 3) (32, 10)\n",
      "61 (32, 64, 64, 3) (32, 10)\n",
      "62 (32, 64, 64, 3) (32, 10)\n",
      "63 (32, 64, 64, 3) (32, 10)\n",
      "64 (32, 64, 64, 3) (32, 10)\n",
      "65 (32, 64, 64, 3) (32, 10)\n",
      "66 (32, 64, 64, 3) (32, 10)\n",
      "67 (32, 64, 64, 3) (32, 10)\n",
      "68 (32, 64, 64, 3) (32, 10)\n",
      "69 (32, 64, 64, 3) (32, 10)\n",
      "70 (32, 64, 64, 3) (32, 10)\n",
      "71 (32, 64, 64, 3) (32, 10)\n",
      "72 (32, 64, 64, 3) (32, 10)\n",
      "73 (32, 64, 64, 3) (32, 10)\n",
      "74 (32, 64, 64, 3) (32, 10)\n",
      "75 (32, 64, 64, 3) (32, 10)\n",
      "76 (32, 64, 64, 3) (32, 10)\n",
      "77 (32, 64, 64, 3) (32, 10)\n",
      "78 (32, 64, 64, 3) (32, 10)\n",
      "79 (32, 64, 64, 3) (32, 10)\n",
      "80 (32, 64, 64, 3) (32, 10)\n",
      "81 (32, 64, 64, 3) (32, 10)\n",
      "82 (32, 64, 64, 3) (32, 10)\n",
      "83 (32, 64, 64, 3) (32, 10)\n",
      "84 (32, 64, 64, 3) (32, 10)\n",
      "85 (32, 64, 64, 3) (32, 10)\n",
      "86 (32, 64, 64, 3) (32, 10)\n",
      "87 (32, 64, 64, 3) (32, 10)\n",
      "88 (32, 64, 64, 3) (32, 10)\n",
      "89 (32, 64, 64, 3) (32, 10)\n",
      "90 (32, 64, 64, 3) (32, 10)\n",
      "91 (32, 64, 64, 3) (32, 10)\n",
      "92 (32, 64, 64, 3) (32, 10)\n",
      "93 (32, 64, 64, 3) (32, 10)\n",
      "94 (32, 64, 64, 3) (32, 10)\n",
      "95 (32, 64, 64, 3) (32, 10)\n",
      "96 (32, 64, 64, 3) (32, 10)\n",
      "97 (32, 64, 64, 3) (32, 10)\n",
      "98 (32, 64, 64, 3) (32, 10)\n",
      "99 (32, 64, 64, 3) (32, 10)\n",
      "100 (32, 64, 64, 3) (32, 10)\n",
      "101 (32, 64, 64, 3) (32, 10)\n",
      "102 (32, 64, 64, 3) (32, 10)\n",
      "103 (32, 64, 64, 3) (32, 10)\n",
      "104 (32, 64, 64, 3) (32, 10)\n",
      "105 (32, 64, 64, 3) (32, 10)\n",
      "106 (32, 64, 64, 3) (32, 10)\n",
      "107 (32, 64, 64, 3) (32, 10)\n",
      "108 (32, 64, 64, 3) (32, 10)\n",
      "109 (32, 64, 64, 3) (32, 10)\n",
      "110 (32, 64, 64, 3) (32, 10)\n",
      "111 (32, 64, 64, 3) (32, 10)\n",
      "112 (32, 64, 64, 3) (32, 10)\n",
      "113 (32, 64, 64, 3) (32, 10)\n",
      "114 (32, 64, 64, 3) (32, 10)\n",
      "115 (32, 64, 64, 3) (32, 10)\n",
      "116 (32, 64, 64, 3) (32, 10)\n",
      "117 (32, 64, 64, 3) (32, 10)\n",
      "118 (32, 64, 64, 3) (32, 10)\n",
      "119 (32, 64, 64, 3) (32, 10)\n",
      "120 (32, 64, 64, 3) (32, 10)\n",
      "121 (32, 64, 64, 3) (32, 10)\n",
      "122 (32, 64, 64, 3) (32, 10)\n",
      "123 (32, 64, 64, 3) (32, 10)\n",
      "124 (32, 64, 64, 3) (32, 10)\n",
      "125 (32, 64, 64, 3) (32, 10)\n",
      "126 (32, 64, 64, 3) (32, 10)\n",
      "127 (32, 64, 64, 3) (32, 10)\n",
      "128 (32, 64, 64, 3) (32, 10)\n",
      "129 (32, 64, 64, 3) (32, 10)\n",
      "130 (32, 64, 64, 3) (32, 10)\n",
      "131 (32, 64, 64, 3) (32, 10)\n",
      "132 (32, 64, 64, 3) (32, 10)\n",
      "133 (32, 64, 64, 3) (32, 10)\n",
      "134 (32, 64, 64, 3) (32, 10)\n",
      "135 (32, 64, 64, 3) (32, 10)\n",
      "136 (32, 64, 64, 3) (32, 10)\n",
      "137 (32, 64, 64, 3) (32, 10)\n",
      "138 (32, 64, 64, 3) (32, 10)\n",
      "139 (32, 64, 64, 3) (32, 10)\n",
      "140 (32, 64, 64, 3) (32, 10)\n",
      "141 (32, 64, 64, 3) (32, 10)\n",
      "142 (32, 64, 64, 3) (32, 10)\n",
      "143 (32, 64, 64, 3) (32, 10)\n",
      "144 (32, 64, 64, 3) (32, 10)\n",
      "145 (32, 64, 64, 3) (32, 10)\n",
      "146 (32, 64, 64, 3) (32, 10)\n",
      "147 (32, 64, 64, 3) (32, 10)\n",
      "148 (32, 64, 64, 3) (32, 10)\n",
      "149 (32, 64, 64, 3) (32, 10)\n",
      "150 (32, 64, 64, 3) (32, 10)\n",
      "151 (32, 64, 64, 3) (32, 10)\n",
      "152 (32, 64, 64, 3) (32, 10)\n",
      "153 (32, 64, 64, 3) (32, 10)\n",
      "154 (32, 64, 64, 3) (32, 10)\n",
      "155 (32, 64, 64, 3) (32, 10)\n",
      "156 (32, 64, 64, 3) (32, 10)\n",
      "157 (32, 64, 64, 3) (32, 10)\n",
      "158 (32, 64, 64, 3) (32, 10)\n",
      "159 (32, 64, 64, 3) (32, 10)\n",
      "160 (32, 64, 64, 3) (32, 10)\n",
      "161 (32, 64, 64, 3) (32, 10)\n",
      "162 (32, 64, 64, 3) (32, 10)\n",
      "163 (32, 64, 64, 3) (32, 10)\n",
      "164 (32, 64, 64, 3) (32, 10)\n",
      "165 (32, 64, 64, 3) (32, 10)\n",
      "166 (32, 64, 64, 3) (32, 10)\n",
      "167 (32, 64, 64, 3) (32, 10)\n",
      "168 (32, 64, 64, 3) (32, 10)\n",
      "169 (32, 64, 64, 3) (32, 10)\n",
      "170 (32, 64, 64, 3) (32, 10)\n",
      "171 (32, 64, 64, 3) (32, 10)\n",
      "172 (32, 64, 64, 3) (32, 10)\n",
      "173 (32, 64, 64, 3) (32, 10)\n",
      "174 (32, 64, 64, 3) (32, 10)\n",
      "175 (32, 64, 64, 3) (32, 10)\n",
      "176 (32, 64, 64, 3) (32, 10)\n",
      "177 (32, 64, 64, 3) (32, 10)\n",
      "178 (32, 64, 64, 3) (32, 10)\n",
      "179 (32, 64, 64, 3) (32, 10)\n",
      "180 (32, 64, 64, 3) (32, 10)\n",
      "181 (32, 64, 64, 3) (32, 10)\n",
      "182 (32, 64, 64, 3) (32, 10)\n",
      "183 (32, 64, 64, 3) (32, 10)\n",
      "184 (32, 64, 64, 3) (32, 10)\n",
      "185 (32, 64, 64, 3) (32, 10)\n",
      "186 (32, 64, 64, 3) (32, 10)\n",
      "187 (32, 64, 64, 3) (32, 10)\n",
      "188 (32, 64, 64, 3) (32, 10)\n",
      "189 (32, 64, 64, 3) (32, 10)\n",
      "190 (32, 64, 64, 3) (32, 10)\n",
      "191 (32, 64, 64, 3) (32, 10)\n",
      "192 (32, 64, 64, 3) (32, 10)\n",
      "193 (32, 64, 64, 3) (32, 10)\n",
      "194 (32, 64, 64, 3) (32, 10)\n",
      "195 (32, 64, 64, 3) (32, 10)\n",
      "196 (32, 64, 64, 3) (32, 10)\n",
      "197 (32, 64, 64, 3) (32, 10)\n",
      "198 (32, 64, 64, 3) (32, 10)\n",
      "199 (32, 64, 64, 3) (32, 10)\n",
      "200 (32, 64, 64, 3) (32, 10)\n",
      "201 (32, 64, 64, 3) (32, 10)\n",
      "202 (32, 64, 64, 3) (32, 10)\n",
      "203 (32, 64, 64, 3) (32, 10)\n",
      "204 (32, 64, 64, 3) (32, 10)\n",
      "205 (32, 64, 64, 3) (32, 10)\n",
      "206 (32, 64, 64, 3) (32, 10)\n",
      "207 (32, 64, 64, 3) (32, 10)\n",
      "208 (32, 64, 64, 3) (32, 10)\n",
      "209 (32, 64, 64, 3) (32, 10)\n",
      "210 (32, 64, 64, 3) (32, 10)\n",
      "211 (32, 64, 64, 3) (32, 10)\n",
      "212 (32, 64, 64, 3) (32, 10)\n",
      "213 (32, 64, 64, 3) (32, 10)\n",
      "214 (32, 64, 64, 3) (32, 10)\n",
      "215 (32, 64, 64, 3) (32, 10)\n",
      "216 (32, 64, 64, 3) (32, 10)\n",
      "217 (32, 64, 64, 3) (32, 10)\n",
      "218 (32, 64, 64, 3) (32, 10)\n",
      "219 (32, 64, 64, 3) (32, 10)\n",
      "220 (32, 64, 64, 3) (32, 10)\n",
      "221 (32, 64, 64, 3) (32, 10)\n",
      "222 (32, 64, 64, 3) (32, 10)\n",
      "223 (32, 64, 64, 3) (32, 10)\n",
      "224 (32, 64, 64, 3) (32, 10)\n",
      "225 (32, 64, 64, 3) (32, 10)\n",
      "226 (32, 64, 64, 3) (32, 10)\n",
      "227 (32, 64, 64, 3) (32, 10)\n",
      "228 (32, 64, 64, 3) (32, 10)\n",
      "229 (32, 64, 64, 3) (32, 10)\n",
      "230 (32, 64, 64, 3) (32, 10)\n",
      "231 (32, 64, 64, 3) (32, 10)\n",
      "232 (32, 64, 64, 3) (32, 10)\n",
      "233 (32, 64, 64, 3) (32, 10)\n",
      "234 (32, 64, 64, 3) (32, 10)\n",
      "235 (32, 64, 64, 3) (32, 10)\n",
      "236 (32, 64, 64, 3) (32, 10)\n",
      "237 (32, 64, 64, 3) (32, 10)\n",
      "238 (32, 64, 64, 3) (32, 10)\n",
      "239 (32, 64, 64, 3) (32, 10)\n",
      "240 (32, 64, 64, 3) (32, 10)\n",
      "241 (32, 64, 64, 3) (32, 10)\n",
      "242 (32, 64, 64, 3) (32, 10)\n",
      "243 (32, 64, 64, 3) (32, 10)\n",
      "244 (32, 64, 64, 3) (32, 10)\n",
      "245 (32, 64, 64, 3) (32, 10)\n",
      "246 (32, 64, 64, 3) (32, 10)\n",
      "247 (32, 64, 64, 3) (32, 10)\n",
      "248 (32, 64, 64, 3) (32, 10)\n",
      "249 (32, 64, 64, 3) (32, 10)\n",
      "250 (32, 64, 64, 3) (32, 10)\n",
      "251 (32, 64, 64, 3) (32, 10)\n",
      "252 (32, 64, 64, 3) (32, 10)\n",
      "253 (32, 64, 64, 3) (32, 10)\n",
      "254 (32, 64, 64, 3) (32, 10)\n",
      "255 (32, 64, 64, 3) (32, 10)\n",
      "256 (32, 64, 64, 3) (32, 10)\n",
      "257 (32, 64, 64, 3) (32, 10)\n",
      "258 (32, 64, 64, 3) (32, 10)\n",
      "259 (32, 64, 64, 3) (32, 10)\n",
      "260 (32, 64, 64, 3) (32, 10)\n",
      "261 (32, 64, 64, 3) (32, 10)\n",
      "262 (32, 64, 64, 3) (32, 10)\n",
      "263 (32, 64, 64, 3) (32, 10)\n",
      "264 (32, 64, 64, 3) (32, 10)\n",
      "265 (32, 64, 64, 3) (32, 10)\n",
      "266 (32, 64, 64, 3) (32, 10)\n",
      "267 (32, 64, 64, 3) (32, 10)\n",
      "268 (32, 64, 64, 3) (32, 10)\n",
      "269 (32, 64, 64, 3) (32, 10)\n",
      "270 (32, 64, 64, 3) (32, 10)\n",
      "271 (32, 64, 64, 3) (32, 10)\n",
      "272 (32, 64, 64, 3) (32, 10)\n",
      "273 (32, 64, 64, 3) (32, 10)\n",
      "274 (32, 64, 64, 3) (32, 10)\n",
      "275 (32, 64, 64, 3) (32, 10)\n",
      "276 (32, 64, 64, 3) (32, 10)\n",
      "277 (32, 64, 64, 3) (32, 10)\n",
      "278 (32, 64, 64, 3) (32, 10)\n",
      "279 (32, 64, 64, 3) (32, 10)\n",
      "280 (32, 64, 64, 3) (32, 10)\n",
      "281 (32, 64, 64, 3) (32, 10)\n",
      "282 (32, 64, 64, 3) (32, 10)\n",
      "283 (32, 64, 64, 3) (32, 10)\n",
      "284 (32, 64, 64, 3) (32, 10)\n",
      "285 (32, 64, 64, 3) (32, 10)\n",
      "286 (32, 64, 64, 3) (32, 10)\n",
      "287 (32, 64, 64, 3) (32, 10)\n",
      "288 (32, 64, 64, 3) (32, 10)\n",
      "289 (32, 64, 64, 3) (32, 10)\n",
      "290 (32, 64, 64, 3) (32, 10)\n",
      "291 (32, 64, 64, 3) (32, 10)\n",
      "292 (32, 64, 64, 3) (32, 10)\n",
      "293 (32, 64, 64, 3) (32, 10)\n",
      "294 (32, 64, 64, 3) (32, 10)\n",
      "295 (32, 64, 64, 3) (32, 10)\n",
      "296 (32, 64, 64, 3) (32, 10)\n",
      "297 (32, 64, 64, 3) (32, 10)\n",
      "298 (32, 64, 64, 3) (32, 10)\n",
      "299 (32, 64, 64, 3) (32, 10)\n",
      "300 (32, 64, 64, 3) (32, 10)\n",
      "301 (32, 64, 64, 3) (32, 10)\n",
      "302 (32, 64, 64, 3) (32, 10)\n",
      "303 (32, 64, 64, 3) (32, 10)\n",
      "304 (32, 64, 64, 3) (32, 10)\n",
      "305 (32, 64, 64, 3) (32, 10)\n",
      "306 (32, 64, 64, 3) (32, 10)\n",
      "307 (32, 64, 64, 3) (32, 10)\n",
      "308 (32, 64, 64, 3) (32, 10)\n",
      "309 (32, 64, 64, 3) (32, 10)\n",
      "310 (32, 64, 64, 3) (32, 10)\n",
      "311 (32, 64, 64, 3) (32, 10)\n",
      "312 (32, 64, 64, 3) (32, 10)\n",
      "313 (32, 64, 64, 3) (32, 10)\n",
      "314 (32, 64, 64, 3) (32, 10)\n",
      "315 (32, 64, 64, 3) (32, 10)\n",
      "316 (32, 64, 64, 3) (32, 10)\n",
      "317 (32, 64, 64, 3) (32, 10)\n",
      "318 (32, 64, 64, 3) (32, 10)\n",
      "319 (32, 64, 64, 3) (32, 10)\n",
      "320 (32, 64, 64, 3) (32, 10)\n",
      "321 (32, 64, 64, 3) (32, 10)\n",
      "322 (32, 64, 64, 3) (32, 10)\n",
      "323 (32, 64, 64, 3) (32, 10)\n",
      "324 (32, 64, 64, 3) (32, 10)\n",
      "325 (32, 64, 64, 3) (32, 10)\n",
      "326 (32, 64, 64, 3) (32, 10)\n",
      "327 (32, 64, 64, 3) (32, 10)\n",
      "328 (32, 64, 64, 3) (32, 10)\n",
      "329 (32, 64, 64, 3) (32, 10)\n",
      "330 (32, 64, 64, 3) (32, 10)\n",
      "331 (32, 64, 64, 3) (32, 10)\n",
      "332 (32, 64, 64, 3) (32, 10)\n",
      "333 (32, 64, 64, 3) (32, 10)\n",
      "334 (32, 64, 64, 3) (32, 10)\n",
      "335 (32, 64, 64, 3) (32, 10)\n",
      "336 (32, 64, 64, 3) (32, 10)\n",
      "337 (32, 64, 64, 3) (32, 10)\n",
      "338 (32, 64, 64, 3) (32, 10)\n",
      "339 (32, 64, 64, 3) (32, 10)\n",
      "340 (32, 64, 64, 3) (32, 10)\n",
      "341 (32, 64, 64, 3) (32, 10)\n",
      "342 (32, 64, 64, 3) (32, 10)\n",
      "343 (32, 64, 64, 3) (32, 10)\n",
      "344 (32, 64, 64, 3) (32, 10)\n",
      "345 (32, 64, 64, 3) (32, 10)\n",
      "346 (32, 64, 64, 3) (32, 10)\n",
      "347 (32, 64, 64, 3) (32, 10)\n",
      "348 (32, 64, 64, 3) (32, 10)\n",
      "349 (32, 64, 64, 3) (32, 10)\n",
      "350 (32, 64, 64, 3) (32, 10)\n",
      "351 (32, 64, 64, 3) (32, 10)\n",
      "352 (32, 64, 64, 3) (32, 10)\n",
      "353 (32, 64, 64, 3) (32, 10)\n",
      "354 (32, 64, 64, 3) (32, 10)\n",
      "355 (32, 64, 64, 3) (32, 10)\n",
      "356 (32, 64, 64, 3) (32, 10)\n",
      "357 (32, 64, 64, 3) (32, 10)\n",
      "358 (32, 64, 64, 3) (32, 10)\n",
      "359 (32, 64, 64, 3) (32, 10)\n",
      "360 (32, 64, 64, 3) (32, 10)\n",
      "361 (32, 64, 64, 3) (32, 10)\n",
      "362 (32, 64, 64, 3) (32, 10)\n",
      "363 (32, 64, 64, 3) (32, 10)\n",
      "364 (32, 64, 64, 3) (32, 10)\n",
      "365 (32, 64, 64, 3) (32, 10)\n",
      "366 (32, 64, 64, 3) (32, 10)\n",
      "367 (32, 64, 64, 3) (32, 10)\n",
      "368 (32, 64, 64, 3) (32, 10)\n",
      "369 (32, 64, 64, 3) (32, 10)\n",
      "370 (32, 64, 64, 3) (32, 10)\n",
      "371 (32, 64, 64, 3) (32, 10)\n",
      "372 (32, 64, 64, 3) (32, 10)\n",
      "373 (32, 64, 64, 3) (32, 10)\n",
      "374 (32, 64, 64, 3) (32, 10)\n",
      "375 (32, 64, 64, 3) (32, 10)\n",
      "376 (32, 64, 64, 3) (32, 10)\n",
      "377 (32, 64, 64, 3) (32, 10)\n",
      "378 (32, 64, 64, 3) (32, 10)\n",
      "379 (32, 64, 64, 3) (32, 10)\n",
      "380 (32, 64, 64, 3) (32, 10)\n",
      "381 (32, 64, 64, 3) (32, 10)\n",
      "382 (32, 64, 64, 3) (32, 10)\n",
      "383 (32, 64, 64, 3) (32, 10)\n",
      "384 (32, 64, 64, 3) (32, 10)\n",
      "385 (32, 64, 64, 3) (32, 10)\n",
      "386 (32, 64, 64, 3) (32, 10)\n",
      "387 (32, 64, 64, 3) (32, 10)\n",
      "388 (32, 64, 64, 3) (32, 10)\n",
      "389 (32, 64, 64, 3) (32, 10)\n",
      "390 (32, 64, 64, 3) (32, 10)\n",
      "391 (32, 64, 64, 3) (32, 10)\n",
      "392 (32, 64, 64, 3) (32, 10)\n",
      "393 (32, 64, 64, 3) (32, 10)\n",
      "394 (32, 64, 64, 3) (32, 10)\n",
      "395 (32, 64, 64, 3) (32, 10)\n",
      "396 (32, 64, 64, 3) (32, 10)\n",
      "397 (32, 64, 64, 3) (32, 10)\n",
      "398 (32, 64, 64, 3) (32, 10)\n",
      "399 (32, 64, 64, 3) (32, 10)\n",
      "400 (32, 64, 64, 3) (32, 10)\n",
      "401 (32, 64, 64, 3) (32, 10)\n",
      "402 (32, 64, 64, 3) (32, 10)\n",
      "403 (32, 64, 64, 3) (32, 10)\n",
      "404 (32, 64, 64, 3) (32, 10)\n",
      "405 (32, 64, 64, 3) (32, 10)\n",
      "406 (32, 64, 64, 3) (32, 10)\n",
      "407 (32, 64, 64, 3) (32, 10)\n",
      "408 (32, 64, 64, 3) (32, 10)\n",
      "409 (32, 64, 64, 3) (32, 10)\n",
      "410 (32, 64, 64, 3) (32, 10)\n",
      "411 (32, 64, 64, 3) (32, 10)\n",
      "412 (32, 64, 64, 3) (32, 10)\n",
      "413 (32, 64, 64, 3) (32, 10)\n",
      "414 (32, 64, 64, 3) (32, 10)\n",
      "415 (32, 64, 64, 3) (32, 10)\n",
      "416 (32, 64, 64, 3) (32, 10)\n",
      "417 (32, 64, 64, 3) (32, 10)\n",
      "418 (32, 64, 64, 3) (32, 10)\n",
      "419 (32, 64, 64, 3) (32, 10)\n",
      "420 (32, 64, 64, 3) (32, 10)\n",
      "421 (32, 64, 64, 3) (32, 10)\n",
      "422 (32, 64, 64, 3) (32, 10)\n",
      "423 (32, 64, 64, 3) (32, 10)\n",
      "424 (32, 64, 64, 3) (32, 10)\n",
      "425 (32, 64, 64, 3) (32, 10)\n",
      "426 (32, 64, 64, 3) (32, 10)\n",
      "427 (32, 64, 64, 3) (32, 10)\n",
      "428 (32, 64, 64, 3) (32, 10)\n",
      "429 (32, 64, 64, 3) (32, 10)\n",
      "430 (32, 64, 64, 3) (32, 10)\n",
      "431 (32, 64, 64, 3) (32, 10)\n",
      "432 (32, 64, 64, 3) (32, 10)\n",
      "433 (32, 64, 64, 3) (32, 10)\n",
      "434 (32, 64, 64, 3) (32, 10)\n",
      "435 (32, 64, 64, 3) (32, 10)\n",
      "436 (32, 64, 64, 3) (32, 10)\n",
      "437 (32, 64, 64, 3) (32, 10)\n",
      "438 (32, 64, 64, 3) (32, 10)\n",
      "439 (32, 64, 64, 3) (32, 10)\n",
      "440 (32, 64, 64, 3) (32, 10)\n",
      "441 (32, 64, 64, 3) (32, 10)\n",
      "442 (32, 64, 64, 3) (32, 10)\n",
      "443 (32, 64, 64, 3) (32, 10)\n",
      "444 (32, 64, 64, 3) (32, 10)\n",
      "445 (32, 64, 64, 3) (32, 10)\n",
      "446 (32, 64, 64, 3) (32, 10)\n",
      "447 (32, 64, 64, 3) (32, 10)\n",
      "448 (32, 64, 64, 3) (32, 10)\n",
      "449 (32, 64, 64, 3) (32, 10)\n",
      "450 (32, 64, 64, 3) (32, 10)\n",
      "451 (32, 64, 64, 3) (32, 10)\n",
      "452 (32, 64, 64, 3) (32, 10)\n",
      "453 (32, 64, 64, 3) (32, 10)\n",
      "454 (32, 64, 64, 3) (32, 10)\n",
      "455 (32, 64, 64, 3) (32, 10)\n",
      "456 (32, 64, 64, 3) (32, 10)\n",
      "457 (32, 64, 64, 3) (32, 10)\n",
      "458 (32, 64, 64, 3) (32, 10)\n",
      "459 (32, 64, 64, 3) (32, 10)\n",
      "460 (32, 64, 64, 3) (32, 10)\n",
      "461 (32, 64, 64, 3) (32, 10)\n",
      "462 (32, 64, 64, 3) (32, 10)\n",
      "463 (32, 64, 64, 3) (32, 10)\n",
      "464 (32, 64, 64, 3) (32, 10)\n",
      "465 (32, 64, 64, 3) (32, 10)\n",
      "466 (32, 64, 64, 3) (32, 10)\n",
      "467 (32, 64, 64, 3) (32, 10)\n",
      "468 (32, 64, 64, 3) (32, 10)\n",
      "469 (32, 64, 64, 3) (32, 10)\n",
      "470 (32, 64, 64, 3) (32, 10)\n",
      "471 (32, 64, 64, 3) (32, 10)\n",
      "472 (32, 64, 64, 3) (32, 10)\n",
      "473 (32, 64, 64, 3) (32, 10)\n",
      "474 (32, 64, 64, 3) (32, 10)\n",
      "475 (32, 64, 64, 3) (32, 10)\n",
      "476 (32, 64, 64, 3) (32, 10)\n",
      "477 (32, 64, 64, 3) (32, 10)\n",
      "478 (32, 64, 64, 3) (32, 10)\n",
      "479 (32, 64, 64, 3) (32, 10)\n",
      "480 (32, 64, 64, 3) (32, 10)\n",
      "481 (32, 64, 64, 3) (32, 10)\n",
      "482 (32, 64, 64, 3) (32, 10)\n",
      "483 (32, 64, 64, 3) (32, 10)\n",
      "484 (32, 64, 64, 3) (32, 10)\n",
      "485 (32, 64, 64, 3) (32, 10)\n",
      "486 (32, 64, 64, 3) (32, 10)\n",
      "487 (32, 64, 64, 3) (32, 10)\n",
      "488 (32, 64, 64, 3) (32, 10)\n",
      "489 (32, 64, 64, 3) (32, 10)\n",
      "490 (32, 64, 64, 3) (32, 10)\n",
      "491 (32, 64, 64, 3) (32, 10)\n",
      "492 (32, 64, 64, 3) (32, 10)\n",
      "493 (32, 64, 64, 3) (32, 10)\n",
      "494 (32, 64, 64, 3) (32, 10)\n",
      "495 (32, 64, 64, 3) (32, 10)\n",
      "496 (32, 64, 64, 3) (32, 10)\n",
      "497 (32, 64, 64, 3) (32, 10)\n",
      "498 (32, 64, 64, 3) (32, 10)\n",
      "499 (32, 64, 64, 3) (32, 10)\n",
      "500 (32, 64, 64, 3) (32, 10)\n",
      "501 (32, 64, 64, 3) (32, 10)\n",
      "502 (32, 64, 64, 3) (32, 10)\n",
      "503 (32, 64, 64, 3) (32, 10)\n",
      "504 (32, 64, 64, 3) (32, 10)\n",
      "505 (32, 64, 64, 3) (32, 10)\n",
      "506 (8, 64, 64, 3) (8, 10)\n"
     ]
    }
   ],
   "source": [
    "data_gen_train = DataGenerator(path_images=x_train,\n",
    "                               labels=y_train,\n",
    "                               batch_size=BATCH_SIZE,\n",
    "                               n_classes=n_classes,\n",
    "                               target_size=TARGET_SIZE,\n",
    "                               shuffle=True)\n",
    "\n",
    "data_gen_val = DataGenerator(path_images=x_val,\n",
    "                             labels=y_val,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             n_classes=n_classes,\n",
    "                             target_size=TARGET_SIZE,\n",
    "                             shuffle=False)\n",
    "\n",
    "# For sanity check, let's see the generator's output\n",
    "for i, (x, y) in enumerate(data_gen_train):\n",
    "    print(i, x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "miYBw0ghtcI-"
   },
   "source": [
    "## **Transfer learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "bMZmzlDv1ZoT"
   },
   "outputs": [],
   "source": [
    "n_classes = 10 # For EuroSAT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Muv8wpVz-lS"
   },
   "source": [
    "**1.** Take a pre-trained model as $base\\_model$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "kCuOGWGxstku"
   },
   "outputs": [],
   "source": [
    "vgg16_base = VGG16(include_top=False,\n",
    "                   weights=\"imagenet\",      \n",
    "                   input_shape=(64,64,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVFVwWBC0M0V"
   },
   "source": [
    " **2.** Freeze the $base\\_model$\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "RoKLQDj8z9--"
   },
   "outputs": [],
   "source": [
    "vgg16_base.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Ht6nBKN0Pal"
   },
   "source": [
    " **3.** Add a $head$ (classification layers) to the $base\\_model$\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "Ym84hUE30YCK"
   },
   "outputs": [],
   "source": [
    "input = Input(shape=(64,64,3))\n",
    "x = preprocess_input(input)\n",
    "x = vgg16_base(x)\n",
    "x = Flatten()(vgg16_base.output)\n",
    "x = Dense(100, activation=\"relu\")(x)\n",
    "output = Dense(n_classes, activation=\"softmax\")(x)\n",
    "\n",
    "model = Model(inputs = vgg16_base.inputs, outputs=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xPJ2QJ2R0YJG"
   },
   "source": [
    "**4.** Train the new model: `model.fit(...)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1ltnmnr8) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.005 MB uploaded\\r'), FloatProgress(value=0.18154604683923226, max=1.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">cnn-classification-euroSAT_RGB_Transfer_Learning</strong> at: <a href='https://wandb.ai/nizaanth-raja/CNN%20for%20image%20classification/runs/1ltnmnr8' target=\"_blank\">https://wandb.ai/nizaanth-raja/CNN%20for%20image%20classification/runs/1ltnmnr8</a><br/> View project at: <a href='https://wandb.ai/nizaanth-raja/CNN%20for%20image%20classification' target=\"_blank\">https://wandb.ai/nizaanth-raja/CNN%20for%20image%20classification</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240606_012716-1ltnmnr8\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1ltnmnr8). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f47d1a804a9f43a89151f740ba8a697e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01128888888957186, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Lenovo\\Documents\\Semester_4\\Deep_Learning\\Lecture_slides\\Assignment\\Assignment_05\\wandb\\run-20240606_013042-qmsay7al</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nizaanth-raja/CNN%20for%20image%20classification/runs/qmsay7al' target=\"_blank\">cnn-classification-euroSAT_RGB_Transfer_Learning</a></strong> to <a href='https://wandb.ai/nizaanth-raja/CNN%20for%20image%20classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nizaanth-raja/CNN%20for%20image%20classification' target=\"_blank\">https://wandb.ai/nizaanth-raja/CNN%20for%20image%20classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nizaanth-raja/CNN%20for%20image%20classification/runs/qmsay7al' target=\"_blank\">https://wandb.ai/nizaanth-raja/CNN%20for%20image%20classification/runs/qmsay7al</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Set up the callbacks to be executed during the model training.\n",
    "#       Remember to use Wandb (or another tool) to visualize the model training\n",
    "#       and to share your report.\n",
    "\n",
    "# Callbacks\n",
    "cb_autosave = ModelCheckpoint(\"cnn_eurosat_rgb_model_transfer_learning.keras\",\n",
    "                              mode=\"max\",\n",
    "                              save_best_only=True,\n",
    "                              monitor=\"val_accuracy\",\n",
    "                              verbose=1)\n",
    "\n",
    "cb_early_stop = EarlyStopping(patience=10,\n",
    "                              verbose=1,\n",
    "                              mode=\"auto\",\n",
    "                              monitor=\"val_accuracy\")\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"CNN for image classification\",\n",
    "    name=\"cnn-classification-euroSAT_RGB_Transfer_Learning\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"architecture\": \"CNN\",\n",
    "    \"dataset\": \"EuroSAT_RGB\",\n",
    "    \"bs\": BATCH_SIZE\n",
    "    }\n",
    ")\n",
    "\n",
    "cb_wandb = WandbMetricsLogger()\n",
    "\n",
    "callbacks = [cb_autosave, cb_early_stop, cb_wandb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "507/507 [==============================] - ETA: 0s - loss: 0.7058 - accuracy: 0.7590\n",
      "Epoch 1: val_accuracy improved from -inf to 0.82704, saving model to cnn_eurosat_rgb_model_transfer_learning.keras\n",
      "507/507 [==============================] - 379s 747ms/step - loss: 0.7058 - accuracy: 0.7590 - val_loss: 0.5121 - val_accuracy: 0.8270\n",
      "Epoch 2/40\n",
      "507/507 [==============================] - ETA: 0s - loss: 0.4556 - accuracy: 0.8435\n",
      "Epoch 2: val_accuracy improved from 0.82704 to 0.83315, saving model to cnn_eurosat_rgb_model_transfer_learning.keras\n",
      "507/507 [==============================] - 367s 725ms/step - loss: 0.4556 - accuracy: 0.8435 - val_loss: 0.4649 - val_accuracy: 0.8331\n",
      "Epoch 3/40\n",
      "507/507 [==============================] - ETA: 0s - loss: 0.3825 - accuracy: 0.8655\n",
      "Epoch 3: val_accuracy did not improve from 0.83315\n",
      "507/507 [==============================] - 365s 721ms/step - loss: 0.3825 - accuracy: 0.8655 - val_loss: 0.4719 - val_accuracy: 0.8331\n",
      "Epoch 4/40\n",
      "507/507 [==============================] - ETA: 0s - loss: 0.3423 - accuracy: 0.8822\n",
      "Epoch 4: val_accuracy improved from 0.83315 to 0.85667, saving model to cnn_eurosat_rgb_model_transfer_learning.keras\n",
      "507/507 [==============================] - 366s 723ms/step - loss: 0.3423 - accuracy: 0.8822 - val_loss: 0.4208 - val_accuracy: 0.8567\n",
      "Epoch 5/40\n",
      "507/507 [==============================] - ETA: 0s - loss: 0.3175 - accuracy: 0.8883\n",
      "Epoch 5: val_accuracy improved from 0.85667 to 0.86407, saving model to cnn_eurosat_rgb_model_transfer_learning.keras\n",
      "507/507 [==============================] - 365s 720ms/step - loss: 0.3175 - accuracy: 0.8883 - val_loss: 0.3988 - val_accuracy: 0.8641\n",
      "Epoch 6/40\n",
      "507/507 [==============================] - ETA: 0s - loss: 0.2930 - accuracy: 0.8965\n",
      "Epoch 6: val_accuracy did not improve from 0.86407\n",
      "507/507 [==============================] - 364s 718ms/step - loss: 0.2930 - accuracy: 0.8965 - val_loss: 0.3989 - val_accuracy: 0.8611\n",
      "Epoch 7/40\n",
      "507/507 [==============================] - ETA: 0s - loss: 0.2675 - accuracy: 0.9078\n",
      "Epoch 7: val_accuracy improved from 0.86407 to 0.86481, saving model to cnn_eurosat_rgb_model_transfer_learning.keras\n",
      "507/507 [==============================] - 364s 717ms/step - loss: 0.2675 - accuracy: 0.9078 - val_loss: 0.4029 - val_accuracy: 0.8648\n",
      "Epoch 8/40\n",
      "507/507 [==============================] - ETA: 0s - loss: 0.2567 - accuracy: 0.9124\n",
      "Epoch 8: val_accuracy did not improve from 0.86481\n",
      "507/507 [==============================] - 363s 717ms/step - loss: 0.2567 - accuracy: 0.9124 - val_loss: 0.4153 - val_accuracy: 0.8587\n",
      "Epoch 9/40\n",
      "507/507 [==============================] - ETA: 0s - loss: 0.2349 - accuracy: 0.9185\n",
      "Epoch 9: val_accuracy improved from 0.86481 to 0.86574, saving model to cnn_eurosat_rgb_model_transfer_learning.keras\n",
      "507/507 [==============================] - 370s 730ms/step - loss: 0.2349 - accuracy: 0.9185 - val_loss: 0.4063 - val_accuracy: 0.8657\n",
      "Epoch 10/40\n",
      "507/507 [==============================] - ETA: 0s - loss: 0.2194 - accuracy: 0.9260\n",
      "Epoch 10: val_accuracy did not improve from 0.86574\n",
      "507/507 [==============================] - 364s 717ms/step - loss: 0.2194 - accuracy: 0.9260 - val_loss: 0.3993 - val_accuracy: 0.8654\n",
      "Epoch 11/40\n",
      "507/507 [==============================] - ETA: 0s - loss: 0.2076 - accuracy: 0.9288\n",
      "Epoch 11: val_accuracy improved from 0.86574 to 0.87222, saving model to cnn_eurosat_rgb_model_transfer_learning.keras\n",
      "507/507 [==============================] - 364s 718ms/step - loss: 0.2076 - accuracy: 0.9288 - val_loss: 0.4055 - val_accuracy: 0.8722\n",
      "Epoch 12/40\n",
      "507/507 [==============================] - ETA: 0s - loss: 0.2083 - accuracy: 0.9264\n",
      "Epoch 12: val_accuracy did not improve from 0.87222\n",
      "507/507 [==============================] - 362s 715ms/step - loss: 0.2083 - accuracy: 0.9264 - val_loss: 0.4223 - val_accuracy: 0.8648\n",
      "Epoch 13/40\n",
      "507/507 [==============================] - ETA: 0s - loss: 0.1848 - accuracy: 0.9376\n",
      "Epoch 13: val_accuracy did not improve from 0.87222\n",
      "507/507 [==============================] - 362s 715ms/step - loss: 0.1848 - accuracy: 0.9376 - val_loss: 0.4575 - val_accuracy: 0.8620\n",
      "Epoch 14/40\n",
      "507/507 [==============================] - ETA: 0s - loss: 0.1741 - accuracy: 0.9401\n",
      "Epoch 14: val_accuracy improved from 0.87222 to 0.87611, saving model to cnn_eurosat_rgb_model_transfer_learning.keras\n",
      "507/507 [==============================] - 361s 713ms/step - loss: 0.1741 - accuracy: 0.9401 - val_loss: 0.4104 - val_accuracy: 0.8761\n",
      "Epoch 15/40\n",
      "507/507 [==============================] - ETA: 0s - loss: 0.1658 - accuracy: 0.9436\n",
      "Epoch 15: val_accuracy did not improve from 0.87611\n",
      "507/507 [==============================] - 361s 712ms/step - loss: 0.1658 - accuracy: 0.9436 - val_loss: 0.4288 - val_accuracy: 0.8735\n",
      "Epoch 16/40\n",
      "507/507 [==============================] - ETA: 0s - loss: 0.1622 - accuracy: 0.9466\n",
      "Epoch 16: val_accuracy did not improve from 0.87611\n",
      "507/507 [==============================] - 360s 711ms/step - loss: 0.1622 - accuracy: 0.9466 - val_loss: 0.4193 - val_accuracy: 0.8757\n",
      "Epoch 17/40\n",
      "507/507 [==============================] - ETA: 0s - loss: 0.1495 - accuracy: 0.9497\n",
      "Epoch 17: val_accuracy did not improve from 0.87611\n",
      "507/507 [==============================] - 360s 710ms/step - loss: 0.1495 - accuracy: 0.9497 - val_loss: 0.4521 - val_accuracy: 0.8709\n",
      "Epoch 18/40\n",
      "507/507 [==============================] - ETA: 0s - loss: 0.1453 - accuracy: 0.9516\n",
      "Epoch 18: val_accuracy did not improve from 0.87611\n",
      "507/507 [==============================] - 360s 710ms/step - loss: 0.1453 - accuracy: 0.9516 - val_loss: 0.4631 - val_accuracy: 0.8707\n",
      "Epoch 19/40\n",
      "507/507 [==============================] - ETA: 0s - loss: 0.1356 - accuracy: 0.9560\n",
      "Epoch 19: val_accuracy did not improve from 0.87611\n",
      "507/507 [==============================] - 368s 726ms/step - loss: 0.1356 - accuracy: 0.9560 - val_loss: 0.4456 - val_accuracy: 0.8706\n",
      "Epoch 20/40\n",
      "507/507 [==============================] - ETA: 0s - loss: 0.1351 - accuracy: 0.9544\n",
      "Epoch 20: val_accuracy did not improve from 0.87611\n",
      "507/507 [==============================] - 364s 719ms/step - loss: 0.1351 - accuracy: 0.9544 - val_loss: 0.4800 - val_accuracy: 0.8704\n",
      "Epoch 21/40\n",
      "507/507 [==============================] - ETA: 0s - loss: 0.1276 - accuracy: 0.9573\n",
      "Epoch 21: val_accuracy did not improve from 0.87611\n",
      "507/507 [==============================] - 362s 714ms/step - loss: 0.1276 - accuracy: 0.9573 - val_loss: 0.5057 - val_accuracy: 0.8609\n",
      "Epoch 22/40\n",
      "507/507 [==============================] - ETA: 0s - loss: 0.1188 - accuracy: 0.9614\n",
      "Epoch 22: val_accuracy did not improve from 0.87611\n",
      "507/507 [==============================] - 361s 712ms/step - loss: 0.1188 - accuracy: 0.9614 - val_loss: 0.5025 - val_accuracy: 0.8739\n",
      "Epoch 23/40\n",
      "507/507 [==============================] - ETA: 0s - loss: 0.1197 - accuracy: 0.9595\n",
      "Epoch 23: val_accuracy did not improve from 0.87611\n",
      "507/507 [==============================] - 361s 712ms/step - loss: 0.1197 - accuracy: 0.9595 - val_loss: 0.4891 - val_accuracy: 0.8730\n",
      "Epoch 24/40\n",
      "507/507 [==============================] - ETA: 0s - loss: 0.1112 - accuracy: 0.9636\n",
      "Epoch 24: val_accuracy did not improve from 0.87611\n",
      "507/507 [==============================] - 360s 711ms/step - loss: 0.1112 - accuracy: 0.9636 - val_loss: 0.4862 - val_accuracy: 0.8746\n",
      "Epoch 24: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1ebc2547e90>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(data_gen_train,\n",
    "          epochs=40,\n",
    "          validation_data=data_gen_val,\n",
    "          callbacks=callbacks\n",
    "                    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BsKs_ptO0ZVQ"
   },
   "source": [
    "**5.** $trainable\\_parameters = head\\_parameters $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SQlOBNDC0Zrx",
    "outputId": "21e6afa9-4f25-4e41-a92f-31095eac01be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_18 (InputLayer)       [(None, 64, 64, 3)]       0         \n",
      "                                                                 \n",
      " tf.__operators__.getitem_7  (None, 64, 64, 3)         0         \n",
      "  (SlicingOpLambda)                                              \n",
      "                                                                 \n",
      " tf.nn.bias_add_7 (TFOpLamb  (None, 64, 64, 3)         0         \n",
      " da)                                                             \n",
      "                                                                 \n",
      " vgg16 (Functional)          (None, 2, 2, 512)         14714688  \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 2, 2, 100)         51300     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 2, 2, 10)          1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14766998 (56.33 MB)\n",
      "Trainable params: 52310 (204.34 KB)\n",
      "Non-trainable params: 14714688 (56.13 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "507/507 [==============================] - 259s 511ms/step - loss: 0.0882 - accuracy: 0.9734\n",
      "Validation:\n",
      "169/169 [==============================] - 89s 525ms/step - loss: 0.4862 - accuracy: 0.8746\n",
      "Test:\n",
      "169/169 [==============================] - 89s 525ms/step - loss: 0.4523 - accuracy: 0.8835\n"
     ]
    }
   ],
   "source": [
    "data_gen_test = DataGenerator(path_images=x_test,\n",
    "                              labels=y_test,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              n_classes=n_classes,\n",
    "                              target_size=TARGET_SIZE,\n",
    "                              shuffle=False)\n",
    "\n",
    "print(\"Train:\")\n",
    "scores_train = model.evaluate(data_gen_train)\n",
    "print(\"Validation:\")\n",
    "scores_val = model.evaluate(data_gen_val)\n",
    "print(\"Test:\")\n",
    "scores_test = model.evaluate(data_gen_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J56zeInPzzHG"
   },
   "source": [
    "## **Fine tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JEiq3i_S2T3I"
   },
   "source": [
    "  **1.** Take a pre-trained model as $base\\_model$\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "mzKZ4EXoz2I7"
   },
   "outputs": [],
   "source": [
    "vgg16_base = VGG16(include_top=False,\n",
    "                   weights=\"imagenet\",      \n",
    "                   input_shape=(64,64,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FB9QmbLx2Vsv"
   },
   "source": [
    "**2.** Freeze some layers of the $base\\_model$\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QukiewE12V3S",
    "outputId": "8d0f6e4c-7cde-4b04-f7ad-a921792adf25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_24 (InputLayer)       [(None, 64, 64, 3)]       0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 64, 64, 64)        1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 64, 64, 64)        36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 32, 32, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 32, 32, 128)       73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 32, 32, 128)       147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 16, 16, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 16, 16, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 16, 16, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 8, 8, 512)         1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 8, 8, 512)         2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 8, 8, 512)         2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 4, 4, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14714688 (56.13 MB)\n",
      "Trainable params: 14714688 (56.13 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg16_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aJsx9abB2rFB",
    "outputId": "75302e8b-8f40-4290-9702-d98404a490bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_24 (InputLayer)       [(None, 64, 64, 3)]       0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 64, 64, 64)        1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 64, 64, 64)        36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 32, 32, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 32, 32, 128)       73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 32, 32, 128)       147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 16, 16, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 16, 16, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 16, 16, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 8, 8, 512)         1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 8, 8, 512)         2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 8, 8, 512)         2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 4, 4, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14714688 (56.13 MB)\n",
      "Trainable params: 4719616 (18.00 MB)\n",
      "Non-trainable params: 9995072 (38.13 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg16_base = freeze_up_to(vgg16_base, \"block5_conv2\")\n",
    "vgg16_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KOS9G6oJ2WAi"
   },
   "source": [
    "**3.** Add a $head$ (classification layers) to the $base\\_model$\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "6R77CUye2WHm"
   },
   "outputs": [],
   "source": [
    "#input = Input(shape=(64,64,3))\n",
    "x = preprocess_input(input)\n",
    "x = vgg16_base(x)\n",
    "x = Flatten()(vgg16_base.output)\n",
    "x = Dense(100, activation=\"relu\")(x)\n",
    "output = Dense(n_classes, activation=\"softmax\")(x)\n",
    "\n",
    "model = Model(inputs = vgg16_base.inputs,outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:y4m4na5l) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.014 MB uploaded\\r'), FloatProgress(value=0.06827362426329382, max=1.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>▁▂▃▃▄▅▆▆▆▇██</td></tr><tr><td>epoch/epoch</td><td>▁▂▂▃▄▄▅▅▆▇▇█</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▇▆▆▅▄▃▃▃▂▁▁</td></tr><tr><td>epoch/val_accuracy</td><td>▃▃▃▄▁▆█▅▇▇█▅</td></tr><tr><td>epoch/val_loss</td><td>▃▂▃▁▆▂▁▃▅▄▅█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>0.9529</td></tr><tr><td>epoch/epoch</td><td>11</td></tr><tr><td>epoch/learning_rate</td><td>0.001</td></tr><tr><td>epoch/loss</td><td>0.14344</td></tr><tr><td>epoch/val_accuracy</td><td>0.86815</td></tr><tr><td>epoch/val_loss</td><td>0.47244</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">cnn-classification-euroSAT_RGB_Transfer_Learning</strong> at: <a href='https://wandb.ai/nizaanth-raja/CNN%20for%20image%20classification/runs/y4m4na5l' target=\"_blank\">https://wandb.ai/nizaanth-raja/CNN%20for%20image%20classification/runs/y4m4na5l</a><br/> View project at: <a href='https://wandb.ai/nizaanth-raja/CNN%20for%20image%20classification' target=\"_blank\">https://wandb.ai/nizaanth-raja/CNN%20for%20image%20classification</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240605_201818-y4m4na5l\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:y4m4na5l). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9f9090541624dd4b74943256b7cb294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011111111111111112, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Lenovo\\Documents\\Semester_4\\Deep_Learning\\Lecture_slides\\Assignment\\Assignment_05\\wandb\\run-20240605_220018-8oynq514</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nizaanth-raja/CNN%20for%20image%20classification/runs/8oynq514' target=\"_blank\">cnn-classification-euroSAT_RGB_Fine_Tuning</a></strong> to <a href='https://wandb.ai/nizaanth-raja/CNN%20for%20image%20classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nizaanth-raja/CNN%20for%20image%20classification' target=\"_blank\">https://wandb.ai/nizaanth-raja/CNN%20for%20image%20classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nizaanth-raja/CNN%20for%20image%20classification/runs/8oynq514' target=\"_blank\">https://wandb.ai/nizaanth-raja/CNN%20for%20image%20classification/runs/8oynq514</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Set up the callbacks to be executed during the model training.\n",
    "#       Remember to use Wandb (or another tool) to visualize the model training\n",
    "#       and to share your report.\n",
    "\n",
    "# Callbacks\n",
    "cb_autosave = ModelCheckpoint(\"cnn_eurosat_rgb_model_Fine_Tuning.keras\",\n",
    "                              mode=\"max\",\n",
    "                              save_best_only=True,\n",
    "                              monitor=\"val_accuracy\",\n",
    "                              verbose=1)\n",
    "\n",
    "cb_early_stop = EarlyStopping(patience=10,\n",
    "                              verbose=1,\n",
    "                              mode=\"auto\",\n",
    "                              monitor=\"val_accuracy\")\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"CNN for image classification\",\n",
    "    name=\"cnn-classification-euroSAT_RGB_Fine_Tuning\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"architecture\": \"CNN\",\n",
    "    \"dataset\": \"EuroSAT_RGB\",\n",
    "    \"bs\": BATCH_SIZE\n",
    "    }\n",
    ")\n",
    "\n",
    "cb_wandb = WandbMetricsLogger()\n",
    "\n",
    "callbacks = [cb_autosave, cb_early_stop, cb_wandb]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DAZ18Dcj2WOI"
   },
   "source": [
    "**4.** Train the new model: `model.fit(...)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "507/507 [==============================] - ETA: 0s - loss: 0.6496 - accuracy: 0.7722\n",
      "Epoch 1: val_accuracy improved from -inf to 0.84352, saving model to cnn_eurosat_rgb_model_Fine_Tuning.keras\n",
      "507/507 [==============================] - 461s 909ms/step - loss: 0.6496 - accuracy: 0.7722 - val_loss: 0.4584 - val_accuracy: 0.8435\n",
      "Epoch 2/40\n",
      "507/507 [==============================] - ETA: 0s - loss: 0.3896 - accuracy: 0.8710\n",
      "Epoch 2: val_accuracy improved from 0.84352 to 0.86463, saving model to cnn_eurosat_rgb_model_Fine_Tuning.keras\n",
      "507/507 [==============================] - 461s 910ms/step - loss: 0.3896 - accuracy: 0.8710 - val_loss: 0.4077 - val_accuracy: 0.8646\n",
      "Epoch 3/40\n",
      "507/507 [==============================] - ETA: 0s - loss: 0.3105 - accuracy: 0.8980\n",
      "Epoch 3: val_accuracy improved from 0.86463 to 0.87667, saving model to cnn_eurosat_rgb_model_Fine_Tuning.keras\n",
      "507/507 [==============================] - 468s 924ms/step - loss: 0.3105 - accuracy: 0.8980 - val_loss: 0.3757 - val_accuracy: 0.8767\n",
      "Epoch 4/40\n",
      "507/507 [==============================] - ETA: 0s - loss: 0.2717 - accuracy: 0.9075\n",
      "Epoch 4: val_accuracy improved from 0.87667 to 0.88519, saving model to cnn_eurosat_rgb_model_Fine_Tuning.keras\n",
      "507/507 [==============================] - 470s 928ms/step - loss: 0.2717 - accuracy: 0.9075 - val_loss: 0.3576 - val_accuracy: 0.8852\n",
      "Epoch 5/40\n",
      "507/507 [==============================] - ETA: 0s - loss: 0.2281 - accuracy: 0.9235\n",
      "Epoch 5: val_accuracy did not improve from 0.88519\n",
      "507/507 [==============================] - 466s 920ms/step - loss: 0.2281 - accuracy: 0.9235 - val_loss: 0.4250 - val_accuracy: 0.8733\n",
      "Epoch 6/40\n",
      "507/507 [==============================] - ETA: 0s - loss: 0.2063 - accuracy: 0.9325\n",
      "Epoch 6: val_accuracy did not improve from 0.88519\n",
      "507/507 [==============================] - 479s 945ms/step - loss: 0.2063 - accuracy: 0.9325 - val_loss: 0.4634 - val_accuracy: 0.8580\n",
      "Epoch 7/40\n",
      "507/507 [==============================] - ETA: 0s - loss: 0.1833 - accuracy: 0.9375\n",
      "Epoch 7: val_accuracy improved from 0.88519 to 0.89259, saving model to cnn_eurosat_rgb_model_Fine_Tuning.keras\n",
      "507/507 [==============================] - 449s 886ms/step - loss: 0.1833 - accuracy: 0.9375 - val_loss: 0.3511 - val_accuracy: 0.8926\n",
      "Epoch 8/40\n",
      "507/507 [==============================] - ETA: 0s - loss: 0.1624 - accuracy: 0.9431\n",
      "Epoch 8: val_accuracy did not improve from 0.89259\n",
      "507/507 [==============================] - 447s 882ms/step - loss: 0.1624 - accuracy: 0.9431 - val_loss: 0.4261 - val_accuracy: 0.8798\n",
      "Epoch 9/40\n",
      "507/507 [==============================] - ETA: 0s - loss: 0.1342 - accuracy: 0.9562\n",
      "Epoch 9: val_accuracy improved from 0.89259 to 0.89926, saving model to cnn_eurosat_rgb_model_Fine_Tuning.keras\n",
      "507/507 [==============================] - 453s 893ms/step - loss: 0.1342 - accuracy: 0.9562 - val_loss: 0.3609 - val_accuracy: 0.8993\n",
      "Epoch 10/40\n",
      "507/507 [==============================] - ETA: 0s - loss: 0.1413 - accuracy: 0.9524\n",
      "Epoch 10: val_accuracy improved from 0.89926 to 0.90370, saving model to cnn_eurosat_rgb_model_Fine_Tuning.keras\n",
      "507/507 [==============================] - 445s 878ms/step - loss: 0.1413 - accuracy: 0.9524 - val_loss: 0.3865 - val_accuracy: 0.9037\n",
      "Epoch 11/40\n",
      "507/507 [==============================] - ETA: 0s - loss: 0.1200 - accuracy: 0.9586\n",
      "Epoch 11: val_accuracy did not improve from 0.90370\n",
      "507/507 [==============================] - 452s 893ms/step - loss: 0.1200 - accuracy: 0.9586 - val_loss: 0.4765 - val_accuracy: 0.8969\n",
      "Epoch 12/40\n",
      "507/507 [==============================] - ETA: 0s - loss: 0.1155 - accuracy: 0.9606\n",
      "Epoch 12: val_accuracy did not improve from 0.90370\n",
      "507/507 [==============================] - 450s 888ms/step - loss: 0.1155 - accuracy: 0.9606 - val_loss: 0.5026 - val_accuracy: 0.8822\n",
      "Epoch 13/40\n",
      "507/507 [==============================] - ETA: 0s - loss: 0.1018 - accuracy: 0.9640\n",
      "Epoch 13: val_accuracy did not improve from 0.90370\n",
      "507/507 [==============================] - 446s 879ms/step - loss: 0.1018 - accuracy: 0.9640 - val_loss: 0.4002 - val_accuracy: 0.9006\n",
      "Epoch 14/40\n",
      "507/507 [==============================] - ETA: 0s - loss: 0.1089 - accuracy: 0.9640\n",
      "Epoch 14: val_accuracy did not improve from 0.90370\n",
      "507/507 [==============================] - 445s 879ms/step - loss: 0.1089 - accuracy: 0.9640 - val_loss: 0.4529 - val_accuracy: 0.8919\n",
      "Epoch 15/40\n",
      "507/507 [==============================] - ETA: 0s - loss: 0.0847 - accuracy: 0.9706\n",
      "Epoch 15: val_accuracy improved from 0.90370 to 0.90926, saving model to cnn_eurosat_rgb_model_Fine_Tuning.keras\n",
      "507/507 [==============================] - 445s 879ms/step - loss: 0.0847 - accuracy: 0.9706 - val_loss: 0.4526 - val_accuracy: 0.9093\n",
      "Epoch 16/40\n",
      "507/507 [==============================] - ETA: 0s - loss: 0.0914 - accuracy: 0.9690\n",
      "Epoch 16: val_accuracy did not improve from 0.90926\n",
      "507/507 [==============================] - 450s 888ms/step - loss: 0.0914 - accuracy: 0.9690 - val_loss: 0.4772 - val_accuracy: 0.9054\n",
      "Epoch 17/40\n",
      "507/507 [==============================] - ETA: 0s - loss: 0.0849 - accuracy: 0.9716\n",
      "Epoch 17: val_accuracy did not improve from 0.90926\n",
      "507/507 [==============================] - 451s 890ms/step - loss: 0.0849 - accuracy: 0.9716 - val_loss: 0.4965 - val_accuracy: 0.8944\n",
      "Epoch 18/40\n",
      "507/507 [==============================] - ETA: 0s - loss: 0.0802 - accuracy: 0.9735\n",
      "Epoch 18: val_accuracy did not improve from 0.90926\n",
      "507/507 [==============================] - 446s 880ms/step - loss: 0.0802 - accuracy: 0.9735 - val_loss: 0.4864 - val_accuracy: 0.9076\n",
      "Epoch 19/40\n",
      "507/507 [==============================] - ETA: 0s - loss: 0.0861 - accuracy: 0.9717\n",
      "Epoch 19: val_accuracy did not improve from 0.90926\n",
      "507/507 [==============================] - 452s 893ms/step - loss: 0.0861 - accuracy: 0.9717 - val_loss: 0.5654 - val_accuracy: 0.8969\n",
      "Epoch 20/40\n",
      "507/507 [==============================] - ETA: 0s - loss: 0.0702 - accuracy: 0.9772\n",
      "Epoch 20: val_accuracy did not improve from 0.90926\n",
      "507/507 [==============================] - 447s 882ms/step - loss: 0.0702 - accuracy: 0.9772 - val_loss: 0.4539 - val_accuracy: 0.9028\n",
      "Epoch 21/40\n",
      "507/507 [==============================] - ETA: 0s - loss: 0.0632 - accuracy: 0.9791\n",
      "Epoch 21: val_accuracy did not improve from 0.90926\n",
      "507/507 [==============================] - 448s 883ms/step - loss: 0.0632 - accuracy: 0.9791 - val_loss: 0.6281 - val_accuracy: 0.8880\n",
      "Epoch 22/40\n",
      "507/507 [==============================] - ETA: 0s - loss: 0.0786 - accuracy: 0.9750\n",
      "Epoch 22: val_accuracy did not improve from 0.90926\n",
      "507/507 [==============================] - 448s 884ms/step - loss: 0.0786 - accuracy: 0.9750 - val_loss: 0.5477 - val_accuracy: 0.8946\n",
      "Epoch 23/40\n",
      "507/507 [==============================] - ETA: 0s - loss: 0.0696 - accuracy: 0.9767\n",
      "Epoch 23: val_accuracy did not improve from 0.90926\n",
      "507/507 [==============================] - 445s 879ms/step - loss: 0.0696 - accuracy: 0.9767 - val_loss: 0.6675 - val_accuracy: 0.8922\n",
      "Epoch 24/40\n",
      "507/507 [==============================] - ETA: 0s - loss: 0.0880 - accuracy: 0.9716\n",
      "Epoch 24: val_accuracy did not improve from 0.90926\n",
      "507/507 [==============================] - 444s 876ms/step - loss: 0.0880 - accuracy: 0.9716 - val_loss: 0.5589 - val_accuracy: 0.9044\n",
      "Epoch 25/40\n",
      "507/507 [==============================] - ETA: 0s - loss: 0.0570 - accuracy: 0.9807\n",
      "Epoch 25: val_accuracy did not improve from 0.90926\n",
      "507/507 [==============================] - 445s 878ms/step - loss: 0.0570 - accuracy: 0.9807 - val_loss: 0.5643 - val_accuracy: 0.9033\n",
      "Epoch 25: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1eb695e4890>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(data_gen_train,\n",
    "          epochs=40,\n",
    "          validation_data=data_gen_val,\n",
    "          callbacks=callbacks\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "poQeLLQA2bTG"
   },
   "source": [
    "  **5.** $trainable\\_parameters = head\\_parameters + base\\_model\\_parameters $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PpaMeoc62bau",
    "outputId": "d7206aea-d89f-43be-84d2-cefa22d432fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_24 (InputLayer)       [(None, 64, 64, 3)]       0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 64, 64, 64)        1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 64, 64, 64)        36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 32, 32, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 32, 32, 128)       73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 32, 32, 128)       147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 16, 16, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 16, 16, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 16, 16, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 8, 8, 512)         1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 8, 8, 512)         2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 8, 8, 512)         2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 4, 4, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 100)               204900    \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14920598 (56.92 MB)\n",
      "Trainable params: 4925526 (18.79 MB)\n",
      "Non-trainable params: 9995072 (38.13 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "507/507 [==============================] - 299s 590ms/step - loss: 0.0613 - accuracy: 0.9821\n",
      "Validation:\n",
      "169/169 [==============================] - 113s 668ms/step - loss: 0.5643 - accuracy: 0.9033\n",
      "Test:\n",
      "169/169 [==============================] - 114s 677ms/step - loss: 0.5319 - accuracy: 0.9076\n"
     ]
    }
   ],
   "source": [
    "data_gen_test = DataGenerator(path_images=x_test,\n",
    "                              labels=y_test,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              n_classes=n_classes,\n",
    "                              target_size=TARGET_SIZE,\n",
    "                              shuffle=False)\n",
    "\n",
    "print(\"Train:\")\n",
    "scores_train = model.evaluate(data_gen_train)\n",
    "print(\"Validation:\")\n",
    "scores_val = model.evaluate(data_gen_val)\n",
    "print(\"Test:\")\n",
    "scores_test = model.evaluate(data_gen_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that for this task Fine tuning performing better than Transfer\n",
    "Learning, which is expected as in Fine Tuning we don't freeze all the layers of the feature extractor hence the model training or getting adpated better for EuroSAT_RGB Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "ojJa7RUApi3L",
    "o0JRDMT9yuRd",
    "GofutGvjtSOZ",
    "1lHJ0P-gtWmW",
    "kanUzPrltfD4",
    "H0icG9OIvHcc",
    "yYyU6UOwy4kK",
    "EbJM8HtezDHg",
    "miYBw0ghtcI-",
    "J56zeInPzzHG"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
