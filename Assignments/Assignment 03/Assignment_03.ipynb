{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tnpOp1yOWIJT"
   },
   "source": [
    "# **Course**: Deep Learning\n",
    "\n",
    "[<img align=\"right\" width=\"400\" height=\"100\" src=\"https://www.tu-braunschweig.de/typo3conf/ext/tu_braunschweig/Resources/Public/Images/Logos/tu_braunschweig_logo.svg\">](https://www.tu-braunschweig.de/en/)\n",
    "\n",
    "[Mehdi Maboudi](https://www.tu-braunschweig.de/en/igp/staff/mehdi-maboudi) \\([m.maboudi@tu-bs.de](m.maboudi@tu-bs.de)) and [Pedro Achanccaray](https://www.tu-braunschweig.de/en/igp/staff/pedro-diaz) (p.diaz@tu-bs.de)\n",
    "\n",
    "[Technical University of Braunschweig](https://www.tu-braunschweig.de/en/)  \n",
    "[Institute of Geodesy and Photogrammetry](https://www.tu-braunschweig.de/igp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GCZeFSPlAfEh"
   },
   "source": [
    "# **Assignment 03:** Vanilla CNN for EuroSAT dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZcOA6hVa1PEE"
   },
   "source": [
    "## **Load packages and data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "pt9G0y1g6iA6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in c:\\users\\disha\\anaconda3\\lib\\site-packages (0.16.6)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from wandb) (8.0.4)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from wandb) (5.9.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from wandb) (2.0.1)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\disha\\anaconda3\\lib\\site-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: setproctitle in c:\\users\\disha\\anaconda3\\lib\\site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\disha\\anaconda3\\lib\\site-packages (from wandb) (68.0.0)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from wandb) (4.25.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\disha\\anaconda3\\lib\\site-packages (from Click!=8.0.0,>=7.1->wandb) (0.4.6)\n",
      "Requirement already satisfied: six>=1.4.0 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\disha\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ZzWTOEJ96kT7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\Disha\\.netrc\n"
     ]
    }
   ],
   "source": [
    "!wandb login e0889b0251f131d15192dcbea2f1400c34a44bff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "EC09utecuSt8"
   },
   "outputs": [],
   "source": [
    "# Management of files\n",
    "import os\n",
    "from os.path import exists, join\n",
    "\n",
    "# Tensorflow and Keras\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, \\\n",
    "                                       EarlyStopping\n",
    "\n",
    "# Monitor training\n",
    "import wandb\n",
    "from wandb.keras import WandbMetricsLogger\n",
    "\n",
    "# Working with arrays\n",
    "import numpy as np\n",
    "\n",
    "# External files with functions to load the dataset,\n",
    "# create a CNN model, and a data generator.\n",
    "from importlib import reload\n",
    "import datasets\n",
    "import models\n",
    "import data_generator\n",
    "# Useful to reload modified external files without need\n",
    "# of restarting the kernel. Just run again this cell.\n",
    "reload(datasets)\n",
    "reload(models)\n",
    "reload(data_generator)\n",
    "\n",
    "from datasets import *\n",
    "from models import *\n",
    "from data_generator import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pgp2I7Y0CIuK"
   },
   "source": [
    "**Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "fg3h0LhMCKX3"
   },
   "outputs": [],
   "source": [
    "PROJECT_DIR = \".\" # os.getcwd()\n",
    "SEED = 42\n",
    "BATCH_SIZE = 32\n",
    "TARGET_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TkaArXxnKH8L"
   },
   "source": [
    "### **Download the dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zrFp7ZE7ikFj"
   },
   "source": [
    "We will use the [EuroSAT dataset](https://zenodo.org/record/7711810#.ZFn-y3bP1D9) with Sentinel-2 images. There are two versions of this dataset: RGB (3 bands) and MS (multispectral - 13 bands).\n",
    "\n",
    "For this assignment, we will work with the RGB version. The following lines download the dataset (~130 MB):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 194065,
     "status": "ok",
     "timestamp": 1683539973712,
     "user": {
      "displayName": "PEDRO MARCO ACHANCCARAY DIAZ",
      "userId": "12035005979643113495"
     },
     "user_tz": -120
    },
    "id": "0QH0mmClSjOA",
    "outputId": "ee6d3b76-7a0e-4b63-b763-8a8a296441f6"
   },
   "outputs": [],
   "source": [
    "url_dataset = \"https://zenodo.org/record/7711810/files/EuroSAT_RGB.zip?download=1\"\n",
    "filename = \"EuroSAT_RGB.zip\"\n",
    "\n",
    "if not exists(\"EuroSAT_RGB\"):\n",
    "  !pip install wget\n",
    "  import wget\n",
    "  f = wget.download(url_dataset, PROJECT_DIR)\n",
    "  import zipfile\n",
    "  with zipfile.ZipFile(filename, \"r\") as zip_ref:\n",
    "    zip_ref.extractall(\".\")\n",
    "  os.remove(join(PROJECT_DIR, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5PFgRKW8KOcF"
   },
   "source": [
    "### **Reading the dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Akfo4HAkKFf"
   },
   "source": [
    "The function **`read_eurosat`** is implemented in the **`datasets.py`** file. The output of this function are a dataframe with information about the image paths and their corresponding classes, and the number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 1653,
     "status": "ok",
     "timestamp": 1683539986068,
     "user": {
      "displayName": "PEDRO MARCO ACHANCCARAY DIAZ",
      "userId": "12035005979643113495"
     },
     "user_tz": -120
    },
    "id": "NgVAoUjHyDEC",
    "outputId": "52626cd4-5721-40f5-d51a-2ee032896766"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path_image</th>\n",
       "      <th>class_str</th>\n",
       "      <th>class_int</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.\\EuroSAT_RGB\\Forest\\Forest_2313.jpg</td>\n",
       "      <td>Forest</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>.\\EuroSAT_RGB\\PermanentCrop\\PermanentCrop_2358...</td>\n",
       "      <td>PermanentCrop</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>.\\EuroSAT_RGB\\HerbaceousVegetation\\HerbaceousV...</td>\n",
       "      <td>HerbaceousVegetation</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>.\\EuroSAT_RGB\\Pasture\\Pasture_1415.jpg</td>\n",
       "      <td>Pasture</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.\\EuroSAT_RGB\\Highway\\Highway_1611.jpg</td>\n",
       "      <td>Highway</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26995</th>\n",
       "      <td>.\\EuroSAT_RGB\\River\\River_76.jpg</td>\n",
       "      <td>River</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26996</th>\n",
       "      <td>.\\EuroSAT_RGB\\Forest\\Forest_2391.jpg</td>\n",
       "      <td>Forest</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26997</th>\n",
       "      <td>.\\EuroSAT_RGB\\AnnualCrop\\AnnualCrop_861.jpg</td>\n",
       "      <td>AnnualCrop</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26998</th>\n",
       "      <td>.\\EuroSAT_RGB\\Pasture\\Pasture_1796.jpg</td>\n",
       "      <td>Pasture</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26999</th>\n",
       "      <td>.\\EuroSAT_RGB\\River\\River_2155.jpg</td>\n",
       "      <td>River</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              path_image  \\\n",
       "0                   .\\EuroSAT_RGB\\Forest\\Forest_2313.jpg   \n",
       "1      .\\EuroSAT_RGB\\PermanentCrop\\PermanentCrop_2358...   \n",
       "2      .\\EuroSAT_RGB\\HerbaceousVegetation\\HerbaceousV...   \n",
       "3                 .\\EuroSAT_RGB\\Pasture\\Pasture_1415.jpg   \n",
       "4                 .\\EuroSAT_RGB\\Highway\\Highway_1611.jpg   \n",
       "...                                                  ...   \n",
       "26995                   .\\EuroSAT_RGB\\River\\River_76.jpg   \n",
       "26996               .\\EuroSAT_RGB\\Forest\\Forest_2391.jpg   \n",
       "26997        .\\EuroSAT_RGB\\AnnualCrop\\AnnualCrop_861.jpg   \n",
       "26998             .\\EuroSAT_RGB\\Pasture\\Pasture_1796.jpg   \n",
       "26999                 .\\EuroSAT_RGB\\River\\River_2155.jpg   \n",
       "\n",
       "                  class_str  class_int  \n",
       "0                    Forest          1  \n",
       "1             PermanentCrop          6  \n",
       "2      HerbaceousVegetation          2  \n",
       "3                   Pasture          5  \n",
       "4                   Highway          3  \n",
       "...                     ...        ...  \n",
       "26995                 River          8  \n",
       "26996                Forest          1  \n",
       "26997            AnnualCrop          0  \n",
       "26998               Pasture          5  \n",
       "26999                 River          8  \n",
       "\n",
       "[27000 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_data = join(PROJECT_DIR, \"EuroSAT_RGB\")\n",
    "\n",
    "df, n_classes = read_eurosat(path_data=path_data, SEED=SEED)\n",
    "classes = np.unique(df[\"class_str\"].values)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yYxjd9DIPVzG"
   },
   "source": [
    "### **Train, Validation and Test sets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ER9UAoDjPbmA"
   },
   "source": [
    "Create **three disjoint** sets: `train`, `validation` and `test`.\n",
    "\n",
    "Use the following proportions:\n",
    "- `train`: 60%\n",
    "- `validation`: 20%\n",
    "- `test`: 20%\n",
    "\n",
    "Remember to use **stratified sampling** and the given `SEED` for the splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RlddcAmMknuE"
   },
   "source": [
    "For this, **complete the implementation** of the function **`train_val_test_split`** in the file **`datasets.py`**.\n",
    "\n",
    "_Search for the **`TODO:`** comments in the file._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ryjc-_SiyDEC"
   },
   "outputs": [],
   "source": [
    "splits = train_val_test_split(df,\n",
    "                              val_size=0.2,\n",
    "                              test_size=0.2,\n",
    "                              SEED=SEED)\n",
    "\n",
    "x_train = splits[\"x_train\"]\n",
    "y_train = splits[\"y_train\"]\n",
    "x_val = splits[\"x_val\"]\n",
    "y_val = splits[\"y_val\"]\n",
    "x_test = splits[\"x_test\"]\n",
    "y_test = splits[\"y_test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C8IQe24lR4Lx"
   },
   "source": [
    "#### **Class distribution**\n",
    "\n",
    "For **sanity check**, verify the **class distribution** of each set: `train`, `validation` and `test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 203,
     "status": "ok",
     "timestamp": 1683539996522,
     "user": {
      "displayName": "PEDRO MARCO ACHANCCARAY DIAZ",
      "userId": "12035005979643113495"
     },
     "user_tz": -120
    },
    "id": "zhXWoVjrRhFs",
    "outputId": "2498fbee-f600-4ce0-ff87-37eb9d315ce5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples per class - train: [1800 1800 1800 1500 1500 1200 1500 1800 1500 1800]\n",
      "Samples per class - val: [600 600 600 500 500 400 500 600 500 600]\n",
      "Samples per class - test: [600 600 600 500 500 400 500 600 500 600]\n"
     ]
    }
   ],
   "source": [
    "# Number of samples per class\n",
    "_, counts_train = np.unique(y_train, return_counts=True)\n",
    "_, counts_val = np.unique(y_val, return_counts=True)\n",
    "_, counts_test = np.unique(y_test, return_counts=True)\n",
    "\n",
    "print(\"Samples per class - train: {}\".format(counts_train))\n",
    "print(\"Samples per class - val: {}\".format(counts_val))\n",
    "print(\"Samples per class - test: {}\".format(counts_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cxIMxNBkYpCD"
   },
   "source": [
    "## **Data generator**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5EeRD9-Ml22s"
   },
   "source": [
    "Go to the file **`data_generator.py`** and **complete the implementation** of the data generator.\n",
    "\n",
    "_Search for the **`TODO:`** comments in the file._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "IYEtY1ognL7w",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (32, 64, 64, 3) (32, 10)\n",
      "1 (32, 64, 64, 3) (32, 10)\n",
      "2 (32, 64, 64, 3) (32, 10)\n",
      "3 (32, 64, 64, 3) (32, 10)\n",
      "4 (32, 64, 64, 3) (32, 10)\n",
      "5 (32, 64, 64, 3) (32, 10)\n",
      "6 (32, 64, 64, 3) (32, 10)\n",
      "7 (32, 64, 64, 3) (32, 10)\n",
      "8 (32, 64, 64, 3) (32, 10)\n",
      "9 (32, 64, 64, 3) (32, 10)\n",
      "10 (32, 64, 64, 3) (32, 10)\n",
      "11 (32, 64, 64, 3) (32, 10)\n",
      "12 (32, 64, 64, 3) (32, 10)\n",
      "13 (32, 64, 64, 3) (32, 10)\n",
      "14 (32, 64, 64, 3) (32, 10)\n",
      "15 (32, 64, 64, 3) (32, 10)\n",
      "16 (32, 64, 64, 3) (32, 10)\n",
      "17 (32, 64, 64, 3) (32, 10)\n",
      "18 (32, 64, 64, 3) (32, 10)\n",
      "19 (32, 64, 64, 3) (32, 10)\n",
      "20 (32, 64, 64, 3) (32, 10)\n",
      "21 (32, 64, 64, 3) (32, 10)\n",
      "22 (32, 64, 64, 3) (32, 10)\n",
      "23 (32, 64, 64, 3) (32, 10)\n",
      "24 (32, 64, 64, 3) (32, 10)\n",
      "25 (32, 64, 64, 3) (32, 10)\n",
      "26 (32, 64, 64, 3) (32, 10)\n",
      "27 (32, 64, 64, 3) (32, 10)\n",
      "28 (32, 64, 64, 3) (32, 10)\n",
      "29 (32, 64, 64, 3) (32, 10)\n",
      "30 (32, 64, 64, 3) (32, 10)\n",
      "31 (32, 64, 64, 3) (32, 10)\n",
      "32 (32, 64, 64, 3) (32, 10)\n",
      "33 (32, 64, 64, 3) (32, 10)\n",
      "34 (32, 64, 64, 3) (32, 10)\n",
      "35 (32, 64, 64, 3) (32, 10)\n",
      "36 (32, 64, 64, 3) (32, 10)\n",
      "37 (32, 64, 64, 3) (32, 10)\n",
      "38 (32, 64, 64, 3) (32, 10)\n",
      "39 (32, 64, 64, 3) (32, 10)\n",
      "40 (32, 64, 64, 3) (32, 10)\n",
      "41 (32, 64, 64, 3) (32, 10)\n",
      "42 (32, 64, 64, 3) (32, 10)\n",
      "43 (32, 64, 64, 3) (32, 10)\n",
      "44 (32, 64, 64, 3) (32, 10)\n",
      "45 (32, 64, 64, 3) (32, 10)\n",
      "46 (32, 64, 64, 3) (32, 10)\n",
      "47 (32, 64, 64, 3) (32, 10)\n",
      "48 (32, 64, 64, 3) (32, 10)\n",
      "49 (32, 64, 64, 3) (32, 10)\n",
      "50 (32, 64, 64, 3) (32, 10)\n",
      "51 (32, 64, 64, 3) (32, 10)\n",
      "52 (32, 64, 64, 3) (32, 10)\n",
      "53 (32, 64, 64, 3) (32, 10)\n",
      "54 (32, 64, 64, 3) (32, 10)\n",
      "55 (32, 64, 64, 3) (32, 10)\n",
      "56 (32, 64, 64, 3) (32, 10)\n",
      "57 (32, 64, 64, 3) (32, 10)\n",
      "58 (32, 64, 64, 3) (32, 10)\n",
      "59 (32, 64, 64, 3) (32, 10)\n",
      "60 (32, 64, 64, 3) (32, 10)\n",
      "61 (32, 64, 64, 3) (32, 10)\n",
      "62 (32, 64, 64, 3) (32, 10)\n",
      "63 (32, 64, 64, 3) (32, 10)\n",
      "64 (32, 64, 64, 3) (32, 10)\n",
      "65 (32, 64, 64, 3) (32, 10)\n",
      "66 (32, 64, 64, 3) (32, 10)\n",
      "67 (32, 64, 64, 3) (32, 10)\n",
      "68 (32, 64, 64, 3) (32, 10)\n",
      "69 (32, 64, 64, 3) (32, 10)\n",
      "70 (32, 64, 64, 3) (32, 10)\n",
      "71 (32, 64, 64, 3) (32, 10)\n",
      "72 (32, 64, 64, 3) (32, 10)\n",
      "73 (32, 64, 64, 3) (32, 10)\n",
      "74 (32, 64, 64, 3) (32, 10)\n",
      "75 (32, 64, 64, 3) (32, 10)\n",
      "76 (32, 64, 64, 3) (32, 10)\n",
      "77 (32, 64, 64, 3) (32, 10)\n",
      "78 (32, 64, 64, 3) (32, 10)\n",
      "79 (32, 64, 64, 3) (32, 10)\n",
      "80 (32, 64, 64, 3) (32, 10)\n",
      "81 (32, 64, 64, 3) (32, 10)\n",
      "82 (32, 64, 64, 3) (32, 10)\n",
      "83 (32, 64, 64, 3) (32, 10)\n",
      "84 (32, 64, 64, 3) (32, 10)\n",
      "85 (32, 64, 64, 3) (32, 10)\n",
      "86 (32, 64, 64, 3) (32, 10)\n",
      "87 (32, 64, 64, 3) (32, 10)\n",
      "88 (32, 64, 64, 3) (32, 10)\n",
      "89 (32, 64, 64, 3) (32, 10)\n",
      "90 (32, 64, 64, 3) (32, 10)\n",
      "91 (32, 64, 64, 3) (32, 10)\n",
      "92 (32, 64, 64, 3) (32, 10)\n",
      "93 (32, 64, 64, 3) (32, 10)\n",
      "94 (32, 64, 64, 3) (32, 10)\n",
      "95 (32, 64, 64, 3) (32, 10)\n",
      "96 (32, 64, 64, 3) (32, 10)\n",
      "97 (32, 64, 64, 3) (32, 10)\n",
      "98 (32, 64, 64, 3) (32, 10)\n",
      "99 (32, 64, 64, 3) (32, 10)\n",
      "100 (32, 64, 64, 3) (32, 10)\n",
      "101 (32, 64, 64, 3) (32, 10)\n",
      "102 (32, 64, 64, 3) (32, 10)\n",
      "103 (32, 64, 64, 3) (32, 10)\n",
      "104 (32, 64, 64, 3) (32, 10)\n",
      "105 (32, 64, 64, 3) (32, 10)\n",
      "106 (32, 64, 64, 3) (32, 10)\n",
      "107 (32, 64, 64, 3) (32, 10)\n",
      "108 (32, 64, 64, 3) (32, 10)\n",
      "109 (32, 64, 64, 3) (32, 10)\n",
      "110 (32, 64, 64, 3) (32, 10)\n",
      "111 (32, 64, 64, 3) (32, 10)\n",
      "112 (32, 64, 64, 3) (32, 10)\n",
      "113 (32, 64, 64, 3) (32, 10)\n",
      "114 (32, 64, 64, 3) (32, 10)\n",
      "115 (32, 64, 64, 3) (32, 10)\n",
      "116 (32, 64, 64, 3) (32, 10)\n",
      "117 (32, 64, 64, 3) (32, 10)\n",
      "118 (32, 64, 64, 3) (32, 10)\n",
      "119 (32, 64, 64, 3) (32, 10)\n",
      "120 (32, 64, 64, 3) (32, 10)\n",
      "121 (32, 64, 64, 3) (32, 10)\n",
      "122 (32, 64, 64, 3) (32, 10)\n",
      "123 (32, 64, 64, 3) (32, 10)\n",
      "124 (32, 64, 64, 3) (32, 10)\n",
      "125 (32, 64, 64, 3) (32, 10)\n",
      "126 (32, 64, 64, 3) (32, 10)\n",
      "127 (32, 64, 64, 3) (32, 10)\n",
      "128 (32, 64, 64, 3) (32, 10)\n",
      "129 (32, 64, 64, 3) (32, 10)\n",
      "130 (32, 64, 64, 3) (32, 10)\n",
      "131 (32, 64, 64, 3) (32, 10)\n",
      "132 (32, 64, 64, 3) (32, 10)\n",
      "133 (32, 64, 64, 3) (32, 10)\n",
      "134 (32, 64, 64, 3) (32, 10)\n",
      "135 (32, 64, 64, 3) (32, 10)\n",
      "136 (32, 64, 64, 3) (32, 10)\n",
      "137 (32, 64, 64, 3) (32, 10)\n",
      "138 (32, 64, 64, 3) (32, 10)\n",
      "139 (32, 64, 64, 3) (32, 10)\n",
      "140 (32, 64, 64, 3) (32, 10)\n",
      "141 (32, 64, 64, 3) (32, 10)\n",
      "142 (32, 64, 64, 3) (32, 10)\n",
      "143 (32, 64, 64, 3) (32, 10)\n",
      "144 (32, 64, 64, 3) (32, 10)\n",
      "145 (32, 64, 64, 3) (32, 10)\n",
      "146 (32, 64, 64, 3) (32, 10)\n",
      "147 (32, 64, 64, 3) (32, 10)\n",
      "148 (32, 64, 64, 3) (32, 10)\n",
      "149 (32, 64, 64, 3) (32, 10)\n",
      "150 (32, 64, 64, 3) (32, 10)\n",
      "151 (32, 64, 64, 3) (32, 10)\n",
      "152 (32, 64, 64, 3) (32, 10)\n",
      "153 (32, 64, 64, 3) (32, 10)\n",
      "154 (32, 64, 64, 3) (32, 10)\n",
      "155 (32, 64, 64, 3) (32, 10)\n",
      "156 (32, 64, 64, 3) (32, 10)\n",
      "157 (32, 64, 64, 3) (32, 10)\n",
      "158 (32, 64, 64, 3) (32, 10)\n",
      "159 (32, 64, 64, 3) (32, 10)\n",
      "160 (32, 64, 64, 3) (32, 10)\n",
      "161 (32, 64, 64, 3) (32, 10)\n",
      "162 (32, 64, 64, 3) (32, 10)\n",
      "163 (32, 64, 64, 3) (32, 10)\n",
      "164 (32, 64, 64, 3) (32, 10)\n",
      "165 (32, 64, 64, 3) (32, 10)\n",
      "166 (32, 64, 64, 3) (32, 10)\n",
      "167 (32, 64, 64, 3) (32, 10)\n",
      "168 (32, 64, 64, 3) (32, 10)\n",
      "169 (32, 64, 64, 3) (32, 10)\n",
      "170 (32, 64, 64, 3) (32, 10)\n",
      "171 (32, 64, 64, 3) (32, 10)\n",
      "172 (32, 64, 64, 3) (32, 10)\n",
      "173 (32, 64, 64, 3) (32, 10)\n",
      "174 (32, 64, 64, 3) (32, 10)\n",
      "175 (32, 64, 64, 3) (32, 10)\n",
      "176 (32, 64, 64, 3) (32, 10)\n",
      "177 (32, 64, 64, 3) (32, 10)\n",
      "178 (32, 64, 64, 3) (32, 10)\n",
      "179 (32, 64, 64, 3) (32, 10)\n",
      "180 (32, 64, 64, 3) (32, 10)\n",
      "181 (32, 64, 64, 3) (32, 10)\n",
      "182 (32, 64, 64, 3) (32, 10)\n",
      "183 (32, 64, 64, 3) (32, 10)\n",
      "184 (32, 64, 64, 3) (32, 10)\n",
      "185 (32, 64, 64, 3) (32, 10)\n",
      "186 (32, 64, 64, 3) (32, 10)\n",
      "187 (32, 64, 64, 3) (32, 10)\n",
      "188 (32, 64, 64, 3) (32, 10)\n",
      "189 (32, 64, 64, 3) (32, 10)\n",
      "190 (32, 64, 64, 3) (32, 10)\n",
      "191 (32, 64, 64, 3) (32, 10)\n",
      "192 (32, 64, 64, 3) (32, 10)\n",
      "193 (32, 64, 64, 3) (32, 10)\n",
      "194 (32, 64, 64, 3) (32, 10)\n",
      "195 (32, 64, 64, 3) (32, 10)\n",
      "196 (32, 64, 64, 3) (32, 10)\n",
      "197 (32, 64, 64, 3) (32, 10)\n",
      "198 (32, 64, 64, 3) (32, 10)\n",
      "199 (32, 64, 64, 3) (32, 10)\n",
      "200 (32, 64, 64, 3) (32, 10)\n",
      "201 (32, 64, 64, 3) (32, 10)\n",
      "202 (32, 64, 64, 3) (32, 10)\n",
      "203 (32, 64, 64, 3) (32, 10)\n",
      "204 (32, 64, 64, 3) (32, 10)\n",
      "205 (32, 64, 64, 3) (32, 10)\n",
      "206 (32, 64, 64, 3) (32, 10)\n",
      "207 (32, 64, 64, 3) (32, 10)\n",
      "208 (32, 64, 64, 3) (32, 10)\n",
      "209 (32, 64, 64, 3) (32, 10)\n",
      "210 (32, 64, 64, 3) (32, 10)\n",
      "211 (32, 64, 64, 3) (32, 10)\n",
      "212 (32, 64, 64, 3) (32, 10)\n",
      "213 (32, 64, 64, 3) (32, 10)\n",
      "214 (32, 64, 64, 3) (32, 10)\n",
      "215 (32, 64, 64, 3) (32, 10)\n",
      "216 (32, 64, 64, 3) (32, 10)\n",
      "217 (32, 64, 64, 3) (32, 10)\n",
      "218 (32, 64, 64, 3) (32, 10)\n",
      "219 (32, 64, 64, 3) (32, 10)\n",
      "220 (32, 64, 64, 3) (32, 10)\n",
      "221 (32, 64, 64, 3) (32, 10)\n",
      "222 (32, 64, 64, 3) (32, 10)\n",
      "223 (32, 64, 64, 3) (32, 10)\n",
      "224 (32, 64, 64, 3) (32, 10)\n",
      "225 (32, 64, 64, 3) (32, 10)\n",
      "226 (32, 64, 64, 3) (32, 10)\n",
      "227 (32, 64, 64, 3) (32, 10)\n",
      "228 (32, 64, 64, 3) (32, 10)\n",
      "229 (32, 64, 64, 3) (32, 10)\n",
      "230 (32, 64, 64, 3) (32, 10)\n",
      "231 (32, 64, 64, 3) (32, 10)\n",
      "232 (32, 64, 64, 3) (32, 10)\n",
      "233 (32, 64, 64, 3) (32, 10)\n",
      "234 (32, 64, 64, 3) (32, 10)\n",
      "235 (32, 64, 64, 3) (32, 10)\n",
      "236 (32, 64, 64, 3) (32, 10)\n",
      "237 (32, 64, 64, 3) (32, 10)\n",
      "238 (32, 64, 64, 3) (32, 10)\n",
      "239 (32, 64, 64, 3) (32, 10)\n",
      "240 (32, 64, 64, 3) (32, 10)\n",
      "241 (32, 64, 64, 3) (32, 10)\n",
      "242 (32, 64, 64, 3) (32, 10)\n",
      "243 (32, 64, 64, 3) (32, 10)\n",
      "244 (32, 64, 64, 3) (32, 10)\n",
      "245 (32, 64, 64, 3) (32, 10)\n",
      "246 (32, 64, 64, 3) (32, 10)\n",
      "247 (32, 64, 64, 3) (32, 10)\n",
      "248 (32, 64, 64, 3) (32, 10)\n",
      "249 (32, 64, 64, 3) (32, 10)\n",
      "250 (32, 64, 64, 3) (32, 10)\n",
      "251 (32, 64, 64, 3) (32, 10)\n",
      "252 (32, 64, 64, 3) (32, 10)\n",
      "253 (32, 64, 64, 3) (32, 10)\n",
      "254 (32, 64, 64, 3) (32, 10)\n",
      "255 (32, 64, 64, 3) (32, 10)\n",
      "256 (32, 64, 64, 3) (32, 10)\n",
      "257 (32, 64, 64, 3) (32, 10)\n",
      "258 (32, 64, 64, 3) (32, 10)\n",
      "259 (32, 64, 64, 3) (32, 10)\n",
      "260 (32, 64, 64, 3) (32, 10)\n",
      "261 (32, 64, 64, 3) (32, 10)\n",
      "262 (32, 64, 64, 3) (32, 10)\n",
      "263 (32, 64, 64, 3) (32, 10)\n",
      "264 (32, 64, 64, 3) (32, 10)\n",
      "265 (32, 64, 64, 3) (32, 10)\n",
      "266 (32, 64, 64, 3) (32, 10)\n",
      "267 (32, 64, 64, 3) (32, 10)\n",
      "268 (32, 64, 64, 3) (32, 10)\n",
      "269 (32, 64, 64, 3) (32, 10)\n",
      "270 (32, 64, 64, 3) (32, 10)\n",
      "271 (32, 64, 64, 3) (32, 10)\n",
      "272 (32, 64, 64, 3) (32, 10)\n",
      "273 (32, 64, 64, 3) (32, 10)\n",
      "274 (32, 64, 64, 3) (32, 10)\n",
      "275 (32, 64, 64, 3) (32, 10)\n",
      "276 (32, 64, 64, 3) (32, 10)\n",
      "277 (32, 64, 64, 3) (32, 10)\n",
      "278 (32, 64, 64, 3) (32, 10)\n",
      "279 (32, 64, 64, 3) (32, 10)\n",
      "280 (32, 64, 64, 3) (32, 10)\n",
      "281 (32, 64, 64, 3) (32, 10)\n",
      "282 (32, 64, 64, 3) (32, 10)\n",
      "283 (32, 64, 64, 3) (32, 10)\n",
      "284 (32, 64, 64, 3) (32, 10)\n",
      "285 (32, 64, 64, 3) (32, 10)\n",
      "286 (32, 64, 64, 3) (32, 10)\n",
      "287 (32, 64, 64, 3) (32, 10)\n",
      "288 (32, 64, 64, 3) (32, 10)\n",
      "289 (32, 64, 64, 3) (32, 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "290 (32, 64, 64, 3) (32, 10)\n",
      "291 (32, 64, 64, 3) (32, 10)\n",
      "292 (32, 64, 64, 3) (32, 10)\n",
      "293 (32, 64, 64, 3) (32, 10)\n",
      "294 (32, 64, 64, 3) (32, 10)\n",
      "295 (32, 64, 64, 3) (32, 10)\n",
      "296 (32, 64, 64, 3) (32, 10)\n",
      "297 (32, 64, 64, 3) (32, 10)\n",
      "298 (32, 64, 64, 3) (32, 10)\n",
      "299 (32, 64, 64, 3) (32, 10)\n",
      "300 (32, 64, 64, 3) (32, 10)\n",
      "301 (32, 64, 64, 3) (32, 10)\n",
      "302 (32, 64, 64, 3) (32, 10)\n",
      "303 (32, 64, 64, 3) (32, 10)\n",
      "304 (32, 64, 64, 3) (32, 10)\n",
      "305 (32, 64, 64, 3) (32, 10)\n",
      "306 (32, 64, 64, 3) (32, 10)\n",
      "307 (32, 64, 64, 3) (32, 10)\n",
      "308 (32, 64, 64, 3) (32, 10)\n",
      "309 (32, 64, 64, 3) (32, 10)\n",
      "310 (32, 64, 64, 3) (32, 10)\n",
      "311 (32, 64, 64, 3) (32, 10)\n",
      "312 (32, 64, 64, 3) (32, 10)\n",
      "313 (32, 64, 64, 3) (32, 10)\n",
      "314 (32, 64, 64, 3) (32, 10)\n",
      "315 (32, 64, 64, 3) (32, 10)\n",
      "316 (32, 64, 64, 3) (32, 10)\n",
      "317 (32, 64, 64, 3) (32, 10)\n",
      "318 (32, 64, 64, 3) (32, 10)\n",
      "319 (32, 64, 64, 3) (32, 10)\n",
      "320 (32, 64, 64, 3) (32, 10)\n",
      "321 (32, 64, 64, 3) (32, 10)\n",
      "322 (32, 64, 64, 3) (32, 10)\n",
      "323 (32, 64, 64, 3) (32, 10)\n",
      "324 (32, 64, 64, 3) (32, 10)\n",
      "325 (32, 64, 64, 3) (32, 10)\n",
      "326 (32, 64, 64, 3) (32, 10)\n",
      "327 (32, 64, 64, 3) (32, 10)\n",
      "328 (32, 64, 64, 3) (32, 10)\n",
      "329 (32, 64, 64, 3) (32, 10)\n",
      "330 (32, 64, 64, 3) (32, 10)\n",
      "331 (32, 64, 64, 3) (32, 10)\n",
      "332 (32, 64, 64, 3) (32, 10)\n",
      "333 (32, 64, 64, 3) (32, 10)\n",
      "334 (32, 64, 64, 3) (32, 10)\n",
      "335 (32, 64, 64, 3) (32, 10)\n",
      "336 (32, 64, 64, 3) (32, 10)\n",
      "337 (32, 64, 64, 3) (32, 10)\n",
      "338 (32, 64, 64, 3) (32, 10)\n",
      "339 (32, 64, 64, 3) (32, 10)\n",
      "340 (32, 64, 64, 3) (32, 10)\n",
      "341 (32, 64, 64, 3) (32, 10)\n",
      "342 (32, 64, 64, 3) (32, 10)\n",
      "343 (32, 64, 64, 3) (32, 10)\n",
      "344 (32, 64, 64, 3) (32, 10)\n",
      "345 (32, 64, 64, 3) (32, 10)\n",
      "346 (32, 64, 64, 3) (32, 10)\n",
      "347 (32, 64, 64, 3) (32, 10)\n",
      "348 (32, 64, 64, 3) (32, 10)\n",
      "349 (32, 64, 64, 3) (32, 10)\n",
      "350 (32, 64, 64, 3) (32, 10)\n",
      "351 (32, 64, 64, 3) (32, 10)\n",
      "352 (32, 64, 64, 3) (32, 10)\n",
      "353 (32, 64, 64, 3) (32, 10)\n",
      "354 (32, 64, 64, 3) (32, 10)\n",
      "355 (32, 64, 64, 3) (32, 10)\n",
      "356 (32, 64, 64, 3) (32, 10)\n",
      "357 (32, 64, 64, 3) (32, 10)\n",
      "358 (32, 64, 64, 3) (32, 10)\n",
      "359 (32, 64, 64, 3) (32, 10)\n",
      "360 (32, 64, 64, 3) (32, 10)\n",
      "361 (32, 64, 64, 3) (32, 10)\n",
      "362 (32, 64, 64, 3) (32, 10)\n",
      "363 (32, 64, 64, 3) (32, 10)\n",
      "364 (32, 64, 64, 3) (32, 10)\n",
      "365 (32, 64, 64, 3) (32, 10)\n",
      "366 (32, 64, 64, 3) (32, 10)\n",
      "367 (32, 64, 64, 3) (32, 10)\n",
      "368 (32, 64, 64, 3) (32, 10)\n",
      "369 (32, 64, 64, 3) (32, 10)\n",
      "370 (32, 64, 64, 3) (32, 10)\n",
      "371 (32, 64, 64, 3) (32, 10)\n",
      "372 (32, 64, 64, 3) (32, 10)\n",
      "373 (32, 64, 64, 3) (32, 10)\n",
      "374 (32, 64, 64, 3) (32, 10)\n",
      "375 (32, 64, 64, 3) (32, 10)\n",
      "376 (32, 64, 64, 3) (32, 10)\n",
      "377 (32, 64, 64, 3) (32, 10)\n",
      "378 (32, 64, 64, 3) (32, 10)\n",
      "379 (32, 64, 64, 3) (32, 10)\n",
      "380 (32, 64, 64, 3) (32, 10)\n",
      "381 (32, 64, 64, 3) (32, 10)\n",
      "382 (32, 64, 64, 3) (32, 10)\n",
      "383 (32, 64, 64, 3) (32, 10)\n",
      "384 (32, 64, 64, 3) (32, 10)\n",
      "385 (32, 64, 64, 3) (32, 10)\n",
      "386 (32, 64, 64, 3) (32, 10)\n",
      "387 (32, 64, 64, 3) (32, 10)\n",
      "388 (32, 64, 64, 3) (32, 10)\n",
      "389 (32, 64, 64, 3) (32, 10)\n",
      "390 (32, 64, 64, 3) (32, 10)\n",
      "391 (32, 64, 64, 3) (32, 10)\n",
      "392 (32, 64, 64, 3) (32, 10)\n",
      "393 (32, 64, 64, 3) (32, 10)\n",
      "394 (32, 64, 64, 3) (32, 10)\n",
      "395 (32, 64, 64, 3) (32, 10)\n",
      "396 (32, 64, 64, 3) (32, 10)\n",
      "397 (32, 64, 64, 3) (32, 10)\n",
      "398 (32, 64, 64, 3) (32, 10)\n",
      "399 (32, 64, 64, 3) (32, 10)\n",
      "400 (32, 64, 64, 3) (32, 10)\n",
      "401 (32, 64, 64, 3) (32, 10)\n",
      "402 (32, 64, 64, 3) (32, 10)\n",
      "403 (32, 64, 64, 3) (32, 10)\n",
      "404 (32, 64, 64, 3) (32, 10)\n",
      "405 (32, 64, 64, 3) (32, 10)\n",
      "406 (32, 64, 64, 3) (32, 10)\n",
      "407 (32, 64, 64, 3) (32, 10)\n",
      "408 (32, 64, 64, 3) (32, 10)\n",
      "409 (32, 64, 64, 3) (32, 10)\n",
      "410 (32, 64, 64, 3) (32, 10)\n",
      "411 (32, 64, 64, 3) (32, 10)\n",
      "412 (32, 64, 64, 3) (32, 10)\n",
      "413 (32, 64, 64, 3) (32, 10)\n",
      "414 (32, 64, 64, 3) (32, 10)\n",
      "415 (32, 64, 64, 3) (32, 10)\n",
      "416 (32, 64, 64, 3) (32, 10)\n",
      "417 (32, 64, 64, 3) (32, 10)\n",
      "418 (32, 64, 64, 3) (32, 10)\n",
      "419 (32, 64, 64, 3) (32, 10)\n",
      "420 (32, 64, 64, 3) (32, 10)\n",
      "421 (32, 64, 64, 3) (32, 10)\n",
      "422 (32, 64, 64, 3) (32, 10)\n",
      "423 (32, 64, 64, 3) (32, 10)\n",
      "424 (32, 64, 64, 3) (32, 10)\n",
      "425 (32, 64, 64, 3) (32, 10)\n",
      "426 (32, 64, 64, 3) (32, 10)\n",
      "427 (32, 64, 64, 3) (32, 10)\n",
      "428 (32, 64, 64, 3) (32, 10)\n",
      "429 (32, 64, 64, 3) (32, 10)\n",
      "430 (32, 64, 64, 3) (32, 10)\n",
      "431 (32, 64, 64, 3) (32, 10)\n",
      "432 (32, 64, 64, 3) (32, 10)\n",
      "433 (32, 64, 64, 3) (32, 10)\n",
      "434 (32, 64, 64, 3) (32, 10)\n",
      "435 (32, 64, 64, 3) (32, 10)\n",
      "436 (32, 64, 64, 3) (32, 10)\n",
      "437 (32, 64, 64, 3) (32, 10)\n",
      "438 (32, 64, 64, 3) (32, 10)\n",
      "439 (32, 64, 64, 3) (32, 10)\n",
      "440 (32, 64, 64, 3) (32, 10)\n",
      "441 (32, 64, 64, 3) (32, 10)\n",
      "442 (32, 64, 64, 3) (32, 10)\n",
      "443 (32, 64, 64, 3) (32, 10)\n",
      "444 (32, 64, 64, 3) (32, 10)\n",
      "445 (32, 64, 64, 3) (32, 10)\n",
      "446 (32, 64, 64, 3) (32, 10)\n",
      "447 (32, 64, 64, 3) (32, 10)\n",
      "448 (32, 64, 64, 3) (32, 10)\n",
      "449 (32, 64, 64, 3) (32, 10)\n",
      "450 (32, 64, 64, 3) (32, 10)\n",
      "451 (32, 64, 64, 3) (32, 10)\n",
      "452 (32, 64, 64, 3) (32, 10)\n",
      "453 (32, 64, 64, 3) (32, 10)\n",
      "454 (32, 64, 64, 3) (32, 10)\n",
      "455 (32, 64, 64, 3) (32, 10)\n",
      "456 (32, 64, 64, 3) (32, 10)\n",
      "457 (32, 64, 64, 3) (32, 10)\n",
      "458 (32, 64, 64, 3) (32, 10)\n",
      "459 (32, 64, 64, 3) (32, 10)\n",
      "460 (32, 64, 64, 3) (32, 10)\n",
      "461 (32, 64, 64, 3) (32, 10)\n",
      "462 (32, 64, 64, 3) (32, 10)\n",
      "463 (32, 64, 64, 3) (32, 10)\n",
      "464 (32, 64, 64, 3) (32, 10)\n",
      "465 (32, 64, 64, 3) (32, 10)\n",
      "466 (32, 64, 64, 3) (32, 10)\n",
      "467 (32, 64, 64, 3) (32, 10)\n",
      "468 (32, 64, 64, 3) (32, 10)\n",
      "469 (32, 64, 64, 3) (32, 10)\n",
      "470 (32, 64, 64, 3) (32, 10)\n",
      "471 (32, 64, 64, 3) (32, 10)\n",
      "472 (32, 64, 64, 3) (32, 10)\n",
      "473 (32, 64, 64, 3) (32, 10)\n",
      "474 (32, 64, 64, 3) (32, 10)\n",
      "475 (32, 64, 64, 3) (32, 10)\n",
      "476 (32, 64, 64, 3) (32, 10)\n",
      "477 (32, 64, 64, 3) (32, 10)\n",
      "478 (32, 64, 64, 3) (32, 10)\n",
      "479 (32, 64, 64, 3) (32, 10)\n",
      "480 (32, 64, 64, 3) (32, 10)\n",
      "481 (32, 64, 64, 3) (32, 10)\n",
      "482 (32, 64, 64, 3) (32, 10)\n",
      "483 (32, 64, 64, 3) (32, 10)\n",
      "484 (32, 64, 64, 3) (32, 10)\n",
      "485 (32, 64, 64, 3) (32, 10)\n",
      "486 (32, 64, 64, 3) (32, 10)\n",
      "487 (32, 64, 64, 3) (32, 10)\n",
      "488 (32, 64, 64, 3) (32, 10)\n",
      "489 (32, 64, 64, 3) (32, 10)\n",
      "490 (32, 64, 64, 3) (32, 10)\n",
      "491 (32, 64, 64, 3) (32, 10)\n",
      "492 (32, 64, 64, 3) (32, 10)\n",
      "493 (32, 64, 64, 3) (32, 10)\n",
      "494 (32, 64, 64, 3) (32, 10)\n",
      "495 (32, 64, 64, 3) (32, 10)\n",
      "496 (32, 64, 64, 3) (32, 10)\n",
      "497 (32, 64, 64, 3) (32, 10)\n",
      "498 (32, 64, 64, 3) (32, 10)\n",
      "499 (32, 64, 64, 3) (32, 10)\n",
      "500 (32, 64, 64, 3) (32, 10)\n",
      "501 (32, 64, 64, 3) (32, 10)\n",
      "502 (32, 64, 64, 3) (32, 10)\n",
      "503 (32, 64, 64, 3) (32, 10)\n",
      "504 (32, 64, 64, 3) (32, 10)\n",
      "505 (32, 64, 64, 3) (32, 10)\n",
      "506 (8, 64, 64, 3) (8, 10)\n"
     ]
    }
   ],
   "source": [
    "data_gen_train = DataGenerator(path_images=x_train,\n",
    "                               labels=y_train,\n",
    "                               batch_size=BATCH_SIZE,\n",
    "                               n_classes=n_classes,\n",
    "                               target_size=TARGET_SIZE,\n",
    "                               shuffle=True)\n",
    "\n",
    "data_gen_val = DataGenerator(path_images=x_val,\n",
    "                             labels=y_val,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             n_classes=n_classes,\n",
    "                             target_size=TARGET_SIZE,\n",
    "                             shuffle=False)\n",
    "\n",
    "# For sanity check, let's see the generator's output\n",
    "for i, (x, y) in enumerate(data_gen_train):\n",
    "    print(i, x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dah4bJlO-rOW"
   },
   "source": [
    "## **Vanilla CNN for image classification**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dAOghIjKwXuo"
   },
   "source": [
    "Go to the file **`models.py`** and **complete the implementation** of the function **`create_cnn`**.\n",
    "\n",
    "Feel free **to create any CNN** with the following layers: `Conv2D`, `MaxPooling2D`, `BatchNormalization`, `Dropout`, `Flatten` and `Dense`.\n",
    "\n",
    "_Search for the **`TODO:`** comments in the file._\n",
    "\n",
    "Then, create a CNN with a given **number of filters** and **filter size**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "B3XKPsGCtfKS"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Disha\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_1                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_2                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8192</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,097,408</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,570</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m896\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │          \u001b[38;5;34m18,496\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_1                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │          \u001b[38;5;34m73,856\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_2                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │             \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8192\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │       \u001b[38;5;34m2,097,408\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │           \u001b[38;5;34m2,570\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,194,122</span> (8.37 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,194,122\u001b[0m (8.37 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,193,674</span> (8.37 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,193,674\u001b[0m (8.37 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> (1.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m448\u001b[0m (1.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Declare the following variables:\n",
    "#       filters: array with the number of filters used in\n",
    "#                each convolutional layer\n",
    "#       k: kernel size\n",
    "filters = [32,64,128]\n",
    "k = 3\n",
    "\n",
    "model = create_cnn(filters=filters,\n",
    "                   k=k,\n",
    "                   input_shape=(TARGET_SIZE,TARGET_SIZE,3),\n",
    "                   n_classes=n_classes)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oXjo2eGu1QHk"
   },
   "source": [
    "### **Model callbacks**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "raK5yvXHx4T8"
   },
   "source": [
    "Define the callbacks for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdishaghosh-applications\u001b[0m (\u001b[33mdishaxgh\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Disha\\TU Braunschweig\\Deep Learning\\Assignment 03\\wandb\\run-20240519_185915-sftegpja</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dishaxgh/CNN%20for%20image%20classification/runs/sftegpja' target=\"_blank\">cnn-classification-euroSAT_RGB</a></strong> to <a href='https://wandb.ai/dishaxgh/CNN%20for%20image%20classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dishaxgh/CNN%20for%20image%20classification' target=\"_blank\">https://wandb.ai/dishaxgh/CNN%20for%20image%20classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dishaxgh/CNN%20for%20image%20classification/runs/sftegpja' target=\"_blank\">https://wandb.ai/dishaxgh/CNN%20for%20image%20classification/runs/sftegpja</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cb_autosave = ModelCheckpoint(\"classification_model.keras\",\n",
    "                              mode=\"max\",\n",
    "                              save_best_only=True,\n",
    "                              monitor=\"val_accuracy\",\n",
    "                              verbose=1)\n",
    "\n",
    "cb_early_stop = EarlyStopping(patience=20,\n",
    "                              verbose=1,\n",
    "                              mode=\"auto\",\n",
    "                              restore_best_weights=\"True\",\n",
    "                              monitor=\"val_accuracy\")\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"CNN for image classification\",\n",
    "    name=\"cnn-classification-euroSAT_RGB\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"architecture\": \"CNN\",\n",
    "    \"dataset\": \"EuroSAT_RGB\",\n",
    "    \"bs\": BATCH_SIZE\n",
    "    }\n",
    ")\n",
    "\n",
    "cb_wandb = WandbMetricsLogger()\n",
    "\n",
    "callbacks = [cb_autosave, cb_early_stop, cb_wandb]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GpN8IibKyV8x"
   },
   "source": [
    "## **Training the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1120541,
     "status": "ok",
     "timestamp": 1683541170109,
     "user": {
      "displayName": "PEDRO MARCO ACHANCCARAY DIAZ",
      "userId": "12035005979643113495"
     },
     "user_tz": -120
    },
    "id": "NIaFw0zRtFNl",
    "outputId": "96d5429d-0e81-4bc2-fb48-e072bb737b22",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Disha\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step - accuracy: 0.4454 - loss: 1.9813\n",
      "Epoch 1: val_accuracy improved from -inf to 0.29000, saving model to classification_model.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Unable to log learning rate.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 223ms/step - accuracy: 0.4455 - loss: 1.9803 - val_accuracy: 0.2900 - val_loss: 3.0862\n",
      "Epoch 2/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy: 0.6119 - loss: 1.1168\n",
      "Epoch 2: val_accuracy improved from 0.29000 to 0.36889, saving model to classification_model.keras\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 203ms/step - accuracy: 0.6119 - loss: 1.1167 - val_accuracy: 0.3689 - val_loss: 3.9625\n",
      "Epoch 3/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy: 0.6645 - loss: 0.9646\n",
      "Epoch 3: val_accuracy improved from 0.36889 to 0.75278, saving model to classification_model.keras\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 203ms/step - accuracy: 0.6646 - loss: 0.9645 - val_accuracy: 0.7528 - val_loss: 0.7110\n",
      "Epoch 4/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.7084 - loss: 0.8125\n",
      "Epoch 4: val_accuracy did not improve from 0.75278\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 201ms/step - accuracy: 0.7084 - loss: 0.8124 - val_accuracy: 0.5102 - val_loss: 1.5729\n",
      "Epoch 5/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - accuracy: 0.7552 - loss: 0.7184\n",
      "Epoch 5: val_accuracy did not improve from 0.75278\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 207ms/step - accuracy: 0.7552 - loss: 0.7184 - val_accuracy: 0.4956 - val_loss: 3.0350\n",
      "Epoch 6/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - accuracy: 0.7821 - loss: 0.6347\n",
      "Epoch 6: val_accuracy did not improve from 0.75278\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 203ms/step - accuracy: 0.7821 - loss: 0.6347 - val_accuracy: 0.6076 - val_loss: 1.5550\n",
      "Epoch 7/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy: 0.8095 - loss: 0.5760\n",
      "Epoch 7: val_accuracy improved from 0.75278 to 0.76389, saving model to classification_model.keras\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 202ms/step - accuracy: 0.8095 - loss: 0.5759 - val_accuracy: 0.7639 - val_loss: 0.7733\n",
      "Epoch 8/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - accuracy: 0.8300 - loss: 0.5018\n",
      "Epoch 8: val_accuracy did not improve from 0.76389\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 203ms/step - accuracy: 0.8300 - loss: 0.5018 - val_accuracy: 0.7235 - val_loss: 1.1134\n",
      "Epoch 9/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 265ms/step - accuracy: 0.8511 - loss: 0.4431\n",
      "Epoch 9: val_accuracy did not improve from 0.76389\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 278ms/step - accuracy: 0.8511 - loss: 0.4431 - val_accuracy: 0.6180 - val_loss: 1.8770\n",
      "Epoch 10/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.8755 - loss: 0.3897\n",
      "Epoch 10: val_accuracy did not improve from 0.76389\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 206ms/step - accuracy: 0.8754 - loss: 0.3898 - val_accuracy: 0.4730 - val_loss: 8.6226\n",
      "Epoch 11/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step - accuracy: 0.8847 - loss: 0.3585\n",
      "Epoch 11: val_accuracy did not improve from 0.76389\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 250ms/step - accuracy: 0.8847 - loss: 0.3585 - val_accuracy: 0.6961 - val_loss: 1.0670\n",
      "Epoch 12/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.9018 - loss: 0.3051\n",
      "Epoch 12: val_accuracy did not improve from 0.76389\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 224ms/step - accuracy: 0.9017 - loss: 0.3051 - val_accuracy: 0.7559 - val_loss: 0.9207\n",
      "Epoch 13/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step - accuracy: 0.9107 - loss: 0.2803\n",
      "Epoch 13: val_accuracy did not improve from 0.76389\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 217ms/step - accuracy: 0.9107 - loss: 0.2803 - val_accuracy: 0.7428 - val_loss: 1.7890\n",
      "Epoch 14/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 265ms/step - accuracy: 0.9221 - loss: 0.2476\n",
      "Epoch 14: val_accuracy improved from 0.76389 to 0.79741, saving model to classification_model.keras\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 293ms/step - accuracy: 0.9221 - loss: 0.2477 - val_accuracy: 0.7974 - val_loss: 0.7080\n",
      "Epoch 15/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 264ms/step - accuracy: 0.9195 - loss: 0.2471\n",
      "Epoch 15: val_accuracy improved from 0.79741 to 0.85019, saving model to classification_model.keras\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 287ms/step - accuracy: 0.9195 - loss: 0.2471 - val_accuracy: 0.8502 - val_loss: 0.5325\n",
      "Epoch 16/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.9304 - loss: 0.2140\n",
      "Epoch 16: val_accuracy did not improve from 0.85019\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 184ms/step - accuracy: 0.9304 - loss: 0.2140 - val_accuracy: 0.7357 - val_loss: 1.1985\n",
      "Epoch 17/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy: 0.9266 - loss: 0.2207\n",
      "Epoch 17: val_accuracy did not improve from 0.85019\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 207ms/step - accuracy: 0.9266 - loss: 0.2207 - val_accuracy: 0.6837 - val_loss: 2.4062\n",
      "Epoch 18/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - accuracy: 0.9385 - loss: 0.1792\n",
      "Epoch 18: val_accuracy did not improve from 0.85019\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 188ms/step - accuracy: 0.9385 - loss: 0.1792 - val_accuracy: 0.6946 - val_loss: 1.8545\n",
      "Epoch 19/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.9368 - loss: 0.1952\n",
      "Epoch 19: val_accuracy did not improve from 0.85019\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 193ms/step - accuracy: 0.9368 - loss: 0.1952 - val_accuracy: 0.5944 - val_loss: 2.8767\n",
      "Epoch 20/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.9459 - loss: 0.1712\n",
      "Epoch 20: val_accuracy improved from 0.85019 to 0.85889, saving model to classification_model.keras\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 175ms/step - accuracy: 0.9459 - loss: 0.1713 - val_accuracy: 0.8589 - val_loss: 0.7384\n",
      "Epoch 21/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy: 0.9450 - loss: 0.1728\n",
      "Epoch 21: val_accuracy did not improve from 0.85889\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 198ms/step - accuracy: 0.9450 - loss: 0.1728 - val_accuracy: 0.7967 - val_loss: 0.9235\n",
      "Epoch 22/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235ms/step - accuracy: 0.9463 - loss: 0.1769\n",
      "Epoch 22: val_accuracy improved from 0.85889 to 0.86130, saving model to classification_model.keras\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 260ms/step - accuracy: 0.9463 - loss: 0.1769 - val_accuracy: 0.8613 - val_loss: 0.7382\n",
      "Epoch 23/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 242ms/step - accuracy: 0.9548 - loss: 0.1424\n",
      "Epoch 23: val_accuracy did not improve from 0.86130\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 263ms/step - accuracy: 0.9548 - loss: 0.1424 - val_accuracy: 0.6769 - val_loss: 1.7966\n",
      "Epoch 24/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 242ms/step - accuracy: 0.9541 - loss: 0.1508\n",
      "Epoch 24: val_accuracy did not improve from 0.86130\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 268ms/step - accuracy: 0.9541 - loss: 0.1508 - val_accuracy: 0.4352 - val_loss: 17.4479\n",
      "Epoch 25/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 268ms/step - accuracy: 0.9565 - loss: 0.1392\n",
      "Epoch 25: val_accuracy improved from 0.86130 to 0.89204, saving model to classification_model.keras\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 291ms/step - accuracy: 0.9565 - loss: 0.1392 - val_accuracy: 0.8920 - val_loss: 0.4431\n",
      "Epoch 26/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - accuracy: 0.9586 - loss: 0.1288\n",
      "Epoch 26: val_accuracy did not improve from 0.89204\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 239ms/step - accuracy: 0.9586 - loss: 0.1288 - val_accuracy: 0.7565 - val_loss: 1.3642\n",
      "Epoch 27/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step - accuracy: 0.9591 - loss: 0.1283\n",
      "Epoch 27: val_accuracy did not improve from 0.89204\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 250ms/step - accuracy: 0.9591 - loss: 0.1283 - val_accuracy: 0.8309 - val_loss: 0.7525\n",
      "Epoch 28/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step - accuracy: 0.9685 - loss: 0.0978\n",
      "Epoch 28: val_accuracy did not improve from 0.89204\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 251ms/step - accuracy: 0.9685 - loss: 0.0979 - val_accuracy: 0.8900 - val_loss: 0.4735\n",
      "Epoch 29/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step - accuracy: 0.9653 - loss: 0.1115\n",
      "Epoch 29: val_accuracy did not improve from 0.89204\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 254ms/step - accuracy: 0.9653 - loss: 0.1116 - val_accuracy: 0.7331 - val_loss: 1.7502\n",
      "Epoch 30/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step - accuracy: 0.9660 - loss: 0.1122\n",
      "Epoch 30: val_accuracy did not improve from 0.89204\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 246ms/step - accuracy: 0.9660 - loss: 0.1122 - val_accuracy: 0.8919 - val_loss: 0.5193\n",
      "Epoch 31/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step - accuracy: 0.9640 - loss: 0.1108\n",
      "Epoch 31: val_accuracy improved from 0.89204 to 0.89759, saving model to classification_model.keras\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 256ms/step - accuracy: 0.9640 - loss: 0.1108 - val_accuracy: 0.8976 - val_loss: 0.5884\n",
      "Epoch 32/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step - accuracy: 0.9651 - loss: 0.1053\n",
      "Epoch 32: val_accuracy did not improve from 0.89759\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 253ms/step - accuracy: 0.9651 - loss: 0.1054 - val_accuracy: 0.8681 - val_loss: 0.6814\n",
      "Epoch 33/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step - accuracy: 0.9651 - loss: 0.1184\n",
      "Epoch 33: val_accuracy did not improve from 0.89759\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 249ms/step - accuracy: 0.9651 - loss: 0.1184 - val_accuracy: 0.7493 - val_loss: 1.9731\n",
      "Epoch 34/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - accuracy: 0.9683 - loss: 0.1100\n",
      "Epoch 34: val_accuracy did not improve from 0.89759\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 225ms/step - accuracy: 0.9683 - loss: 0.1100 - val_accuracy: 0.6194 - val_loss: 3.1114\n",
      "Epoch 35/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - accuracy: 0.9676 - loss: 0.1051\n",
      "Epoch 35: val_accuracy did not improve from 0.89759\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 205ms/step - accuracy: 0.9676 - loss: 0.1051 - val_accuracy: 0.8452 - val_loss: 1.2638\n",
      "Epoch 36/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - accuracy: 0.9699 - loss: 0.0982\n",
      "Epoch 36: val_accuracy did not improve from 0.89759\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 205ms/step - accuracy: 0.9699 - loss: 0.0982 - val_accuracy: 0.8717 - val_loss: 0.5888\n",
      "Epoch 37/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.9711 - loss: 0.1029\n",
      "Epoch 37: val_accuracy did not improve from 0.89759\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 201ms/step - accuracy: 0.9711 - loss: 0.1029 - val_accuracy: 0.8833 - val_loss: 0.6404\n",
      "Epoch 38/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.9736 - loss: 0.0863\n",
      "Epoch 38: val_accuracy did not improve from 0.89759\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 201ms/step - accuracy: 0.9736 - loss: 0.0864 - val_accuracy: 0.7685 - val_loss: 1.2021\n",
      "Epoch 39/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.9772 - loss: 0.0749\n",
      "Epoch 39: val_accuracy did not improve from 0.89759\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 201ms/step - accuracy: 0.9772 - loss: 0.0749 - val_accuracy: 0.7491 - val_loss: 2.1196\n",
      "Epoch 40/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9754 - loss: 0.0886\n",
      "Epoch 40: val_accuracy did not improve from 0.89759\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1708s\u001b[0m 3s/step - accuracy: 0.9754 - loss: 0.0886 - val_accuracy: 0.7633 - val_loss: 1.8161\n",
      "Epoch 41/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207ms/step - accuracy: 0.9726 - loss: 0.0888\n",
      "Epoch 41: val_accuracy did not improve from 0.89759\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 229ms/step - accuracy: 0.9726 - loss: 0.0888 - val_accuracy: 0.8650 - val_loss: 0.7296\n",
      "Epoch 42/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - accuracy: 0.9645 - loss: 0.1134\n",
      "Epoch 42: val_accuracy did not improve from 0.89759\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 229ms/step - accuracy: 0.9645 - loss: 0.1134 - val_accuracy: 0.5837 - val_loss: 8.4367\n",
      "Epoch 43/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step - accuracy: 0.9743 - loss: 0.0990\n",
      "Epoch 43: val_accuracy did not improve from 0.89759\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 213ms/step - accuracy: 0.9743 - loss: 0.0990 - val_accuracy: 0.7207 - val_loss: 2.5783\n",
      "Epoch 44/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - accuracy: 0.9707 - loss: 0.0946\n",
      "Epoch 44: val_accuracy did not improve from 0.89759\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 218ms/step - accuracy: 0.9707 - loss: 0.0946 - val_accuracy: 0.8972 - val_loss: 0.5915\n",
      "Epoch 45/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step - accuracy: 0.9741 - loss: 0.0803\n",
      "Epoch 45: val_accuracy did not improve from 0.89759\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 212ms/step - accuracy: 0.9741 - loss: 0.0803 - val_accuracy: 0.8954 - val_loss: 0.6784\n",
      "Epoch 46/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.9798 - loss: 0.0678\n",
      "Epoch 46: val_accuracy did not improve from 0.89759\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 151ms/step - accuracy: 0.9798 - loss: 0.0678 - val_accuracy: 0.8839 - val_loss: 0.7744\n",
      "Epoch 47/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.9764 - loss: 0.0728\n",
      "Epoch 47: val_accuracy did not improve from 0.89759\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 181ms/step - accuracy: 0.9764 - loss: 0.0728 - val_accuracy: 0.8335 - val_loss: 0.8962\n",
      "Epoch 48/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.9769 - loss: 0.0787\n",
      "Epoch 48: val_accuracy did not improve from 0.89759\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 151ms/step - accuracy: 0.9769 - loss: 0.0787 - val_accuracy: 0.8094 - val_loss: 1.4672\n",
      "Epoch 49/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.9773 - loss: 0.0783\n",
      "Epoch 49: val_accuracy did not improve from 0.89759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 151ms/step - accuracy: 0.9773 - loss: 0.0783 - val_accuracy: 0.8074 - val_loss: 1.4644\n",
      "Epoch 50/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.9788 - loss: 0.0755\n",
      "Epoch 50: val_accuracy improved from 0.89759 to 0.89870, saving model to classification_model.keras\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 154ms/step - accuracy: 0.9787 - loss: 0.0755 - val_accuracy: 0.8987 - val_loss: 0.5843\n",
      "Epoch 51/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.9758 - loss: 0.0751\n",
      "Epoch 51: val_accuracy did not improve from 0.89870\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 152ms/step - accuracy: 0.9758 - loss: 0.0752 - val_accuracy: 0.8819 - val_loss: 0.6349\n",
      "Epoch 52/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step - accuracy: 0.9795 - loss: 0.0743\n",
      "Epoch 52: val_accuracy did not improve from 0.89870\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 216ms/step - accuracy: 0.9795 - loss: 0.0743 - val_accuracy: 0.8761 - val_loss: 0.8359\n",
      "Epoch 53/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step - accuracy: 0.9781 - loss: 0.0791\n",
      "Epoch 53: val_accuracy did not improve from 0.89870\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 238ms/step - accuracy: 0.9781 - loss: 0.0791 - val_accuracy: 0.6913 - val_loss: 3.0619\n",
      "Epoch 54/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - accuracy: 0.9768 - loss: 0.0777\n",
      "Epoch 54: val_accuracy did not improve from 0.89870\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 208ms/step - accuracy: 0.9768 - loss: 0.0777 - val_accuracy: 0.7948 - val_loss: 2.1628\n",
      "Epoch 55/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step - accuracy: 0.9823 - loss: 0.0602\n",
      "Epoch 55: val_accuracy did not improve from 0.89870\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 210ms/step - accuracy: 0.9823 - loss: 0.0602 - val_accuracy: 0.7726 - val_loss: 2.2905\n",
      "Epoch 56/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - accuracy: 0.9782 - loss: 0.0725\n",
      "Epoch 56: val_accuracy did not improve from 0.89870\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 203ms/step - accuracy: 0.9782 - loss: 0.0725 - val_accuracy: 0.8680 - val_loss: 0.7017\n",
      "Epoch 57/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - accuracy: 0.9815 - loss: 0.0595\n",
      "Epoch 57: val_accuracy did not improve from 0.89870\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 204ms/step - accuracy: 0.9815 - loss: 0.0595 - val_accuracy: 0.8676 - val_loss: 0.8258\n",
      "Epoch 58/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy: 0.9841 - loss: 0.0517\n",
      "Epoch 58: val_accuracy did not improve from 0.89870\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 203ms/step - accuracy: 0.9841 - loss: 0.0517 - val_accuracy: 0.8694 - val_loss: 0.6296\n",
      "Epoch 59/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy: 0.9806 - loss: 0.0614\n",
      "Epoch 59: val_accuracy did not improve from 0.89870\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 202ms/step - accuracy: 0.9806 - loss: 0.0614 - val_accuracy: 0.5987 - val_loss: 5.6852\n",
      "Epoch 60/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy: 0.9776 - loss: 0.0877\n",
      "Epoch 60: val_accuracy did not improve from 0.89870\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 202ms/step - accuracy: 0.9776 - loss: 0.0877 - val_accuracy: 0.8948 - val_loss: 0.7875\n",
      "Epoch 61/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - accuracy: 0.9833 - loss: 0.0573\n",
      "Epoch 61: val_accuracy improved from 0.89870 to 0.92056, saving model to classification_model.keras\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 204ms/step - accuracy: 0.9833 - loss: 0.0573 - val_accuracy: 0.9206 - val_loss: 0.5443\n",
      "Epoch 62/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 0.9869 - loss: 0.0450\n",
      "Epoch 62: val_accuracy did not improve from 0.92056\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 199ms/step - accuracy: 0.9869 - loss: 0.0450 - val_accuracy: 0.8624 - val_loss: 0.8583\n",
      "Epoch 63/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.9843 - loss: 0.0580\n",
      "Epoch 63: val_accuracy did not improve from 0.92056\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 152ms/step - accuracy: 0.9843 - loss: 0.0580 - val_accuracy: 0.8752 - val_loss: 0.8858\n",
      "Epoch 64/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.9824 - loss: 0.0625\n",
      "Epoch 64: val_accuracy did not improve from 0.92056\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 155ms/step - accuracy: 0.9824 - loss: 0.0625 - val_accuracy: 0.8711 - val_loss: 0.7988\n",
      "Epoch 65/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.9782 - loss: 0.0769\n",
      "Epoch 65: val_accuracy did not improve from 0.92056\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 154ms/step - accuracy: 0.9782 - loss: 0.0769 - val_accuracy: 0.7213 - val_loss: 3.2942\n",
      "Epoch 66/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.9853 - loss: 0.0546\n",
      "Epoch 66: val_accuracy did not improve from 0.92056\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 155ms/step - accuracy: 0.9853 - loss: 0.0546 - val_accuracy: 0.8987 - val_loss: 0.6204\n",
      "Epoch 67/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.9828 - loss: 0.0627\n",
      "Epoch 67: val_accuracy did not improve from 0.92056\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 155ms/step - accuracy: 0.9828 - loss: 0.0627 - val_accuracy: 0.8389 - val_loss: 1.4222\n",
      "Epoch 68/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.9816 - loss: 0.0549\n",
      "Epoch 68: val_accuracy did not improve from 0.92056\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 183ms/step - accuracy: 0.9816 - loss: 0.0549 - val_accuracy: 0.8463 - val_loss: 1.1662\n",
      "Epoch 69/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.9850 - loss: 0.0498\n",
      "Epoch 69: val_accuracy did not improve from 0.92056\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 161ms/step - accuracy: 0.9850 - loss: 0.0498 - val_accuracy: 0.8437 - val_loss: 1.3848\n",
      "Epoch 70/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.9770 - loss: 0.0845\n",
      "Epoch 70: val_accuracy did not improve from 0.92056\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 156ms/step - accuracy: 0.9770 - loss: 0.0845 - val_accuracy: 0.8387 - val_loss: 1.4393\n",
      "Epoch 71/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.9820 - loss: 0.0718\n",
      "Epoch 71: val_accuracy did not improve from 0.92056\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 155ms/step - accuracy: 0.9820 - loss: 0.0718 - val_accuracy: 0.8750 - val_loss: 0.7044\n",
      "Epoch 72/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.9852 - loss: 0.0534\n",
      "Epoch 72: val_accuracy did not improve from 0.92056\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 156ms/step - accuracy: 0.9852 - loss: 0.0534 - val_accuracy: 0.8928 - val_loss: 0.5832\n",
      "Epoch 73/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.9812 - loss: 0.0679\n",
      "Epoch 73: val_accuracy did not improve from 0.92056\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 155ms/step - accuracy: 0.9812 - loss: 0.0679 - val_accuracy: 0.8728 - val_loss: 0.7517\n",
      "Epoch 74/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.9881 - loss: 0.0472\n",
      "Epoch 74: val_accuracy did not improve from 0.92056\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 154ms/step - accuracy: 0.9881 - loss: 0.0473 - val_accuracy: 0.8081 - val_loss: 1.4347\n",
      "Epoch 75/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.9821 - loss: 0.0704\n",
      "Epoch 75: val_accuracy did not improve from 0.92056\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 155ms/step - accuracy: 0.9821 - loss: 0.0704 - val_accuracy: 0.8956 - val_loss: 0.9166\n",
      "Epoch 76/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.9865 - loss: 0.0484\n",
      "Epoch 76: val_accuracy did not improve from 0.92056\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 155ms/step - accuracy: 0.9865 - loss: 0.0484 - val_accuracy: 0.8361 - val_loss: 1.0653\n",
      "Epoch 77/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.9857 - loss: 0.0490\n",
      "Epoch 77: val_accuracy did not improve from 0.92056\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 156ms/step - accuracy: 0.9857 - loss: 0.0490 - val_accuracy: 0.8763 - val_loss: 0.9349\n",
      "Epoch 78/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.9847 - loss: 0.0498\n",
      "Epoch 78: val_accuracy did not improve from 0.92056\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 158ms/step - accuracy: 0.9847 - loss: 0.0498 - val_accuracy: 0.8850 - val_loss: 0.9917\n",
      "Epoch 79/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.9838 - loss: 0.0662\n",
      "Epoch 79: val_accuracy did not improve from 0.92056\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 157ms/step - accuracy: 0.9838 - loss: 0.0662 - val_accuracy: 0.8509 - val_loss: 1.2355\n",
      "Epoch 80/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.9866 - loss: 0.0473\n",
      "Epoch 80: val_accuracy did not improve from 0.92056\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 156ms/step - accuracy: 0.9866 - loss: 0.0473 - val_accuracy: 0.8806 - val_loss: 1.0089\n",
      "Epoch 81/100\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.9840 - loss: 0.0540\n",
      "Epoch 81: val_accuracy did not improve from 0.92056\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 158ms/step - accuracy: 0.9840 - loss: 0.0540 - val_accuracy: 0.8496 - val_loss: 0.7206\n",
      "Epoch 81: early stopping\n",
      "Restoring model weights from the end of the best epoch: 61.\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(data_gen_train,\n",
    "                    epochs=100,\n",
    "                    validation_data=data_gen_val,\n",
    "                    callbacks=callbacks\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link to the report: https://wandb.ai/dishaxgh/CNN%20for%20image%20classification/reports/Vanilla-CNN--Vmlldzo4MTA1NTU2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38gCzERMCRj3"
   },
   "source": [
    "## **Testing the model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P6F-7iCvygxq"
   },
   "source": [
    "Test your model and report the accuracies for the train, validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "\u001b[1m507/507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 60ms/step - accuracy: 0.9994 - loss: 0.0029\n",
      "Validation:\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 61ms/step - accuracy: 0.9213 - loss: 0.5597\n",
      "Test:\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 60ms/step - accuracy: 0.9207 - loss: 0.6159\n"
     ]
    }
   ],
   "source": [
    "data_gen_test = DataGenerator(path_images=x_test,\n",
    "                              labels=y_test,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              n_classes=n_classes,\n",
    "                              target_size=TARGET_SIZE,\n",
    "                              shuffle=False)\n",
    "\n",
    "print(\"Train:\")\n",
    "scores_train = model.evaluate(data_gen_train)\n",
    "print(\"Validation:\")\n",
    "scores_val = model.evaluate(data_gen_val)\n",
    "print(\"Test:\")\n",
    "scores_test = model.evaluate(data_gen_test)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "EyS5sA5SxSYe",
    "ZcOA6hVa1PEE",
    "cxIMxNBkYpCD",
    "dah4bJlO-rOW",
    "GpN8IibKyV8x",
    "38gCzERMCRj3"
   ],
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
